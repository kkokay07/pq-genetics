{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgYg5AjQx12iBEGaRC4tln",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkokay07/pq-genetics/blob/main/Local%20ancestry%20analysis/LAAX_(Local_Ancestry_Analysis_in_Crossbreds).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Local Ancestry Analysis in Crossbred (LAAX) Notebook - Tips to run\n",
        "\n",
        "This notebook provides a comprehensive pipeline for analyzing local ancestry in crossbred populations. It implements the LAMP (Local Ancestry in adMixed Populations) method to identify genomic regions with significant ancestry deviations that may be associated with selection during breed formation.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, you need:\n",
        "1. A Google account with access to Google Drive\n",
        "2. The following files uploaded to your Google Drive:\n",
        "   - Google drive folder (e.g., `test`) containing binary PLINK files (.bed, .bim, .fam)\n",
        "   - Google drive folder (e.g., `tools_local_ancestry`) containing LAMP executable and other tools\n",
        "\n",
        "## Tips for Running the Notebook\n",
        "\n",
        "- Run cells sequentially from top to bottom\n",
        "- **After running Step 2, check that your parameters were correctly captured**\n",
        "- The notebook will create multiple files throughout the analysis - review outputs at each stage\n",
        "- The final results folder will contain visualizations and reports summarizing your findings\n",
        "\n",
        "\n",
        "## Brief information on steps\n",
        "\n",
        "The notebook is organized in a step-by-step workflow:\n",
        "\n",
        "1. **Configuration Setup** (Step 1-2):\n",
        "   - Mount your Google Drive\n",
        "   - Set folder paths\n",
        "   - Input parameters (ancestral populations, target population, file prefix)\n",
        "\n",
        "2. **Quality Control and Filtering** (Step 3-4):\n",
        "   - Filter SNPs based on MAF, HWE, and missingness\n",
        "   - Perform admixture analysis to identify and remove outlier individuals\n",
        "\n",
        "3. **Ancestry Analysis** (Step 5-7):\n",
        "   - Calculate ancestral allele frequencies\n",
        "   - Prepare LAMP input files\n",
        "   - Run LAMP analysis for each chromosome\n",
        "\n",
        "4. **Results and Visualization** (Step 8-10):\n",
        "   - Calculate delta values (ancestry deviations)\n",
        "   - Identify significant regions\n",
        "   - Create visualizations and final reports\n",
        "\n",
        "\n",
        "## Expected Outputs\n",
        "\n",
        "The analysis will produce:\n",
        "- Admixture visualizations showing population structure\n",
        "- PCA plots showing genetic relationships\n",
        "- Chromosome-specific delta files indicating ancestry deviation\n",
        "- Heatmaps and summary statistics for significant regions\n",
        "- A final HTML report summarizing all findings\n",
        "- Produced folders (final_results, chromosome_plots and significant_regions) in the project folder (where your input files were present)\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "- If a step fails, check the error message and ensure all required files exist\n",
        "- Verify that executables have proper permissions (the notebook attempts to set these)\n",
        "- For large datasets, some steps may take considerable time - be patient\n"
      ],
      "metadata": {
        "id": "QLEaPPb5X3GX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEAjnX0HB75g"
      },
      "outputs": [],
      "source": [
        "# Step 1: Mount Google Drive, set folder paths, and set permissions for all contents\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount your Google Drive (you'll be prompted to authorize)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define your folder paths (adjust these paths based on your Drive structure)\n",
        "PROJECT_FOLDER = '/content/drive/MyDrive/test'\n",
        "TOOLS_FOLDER = '/content/drive/MyDrive/tools_local_ancestry'\n",
        "\n",
        "# Check that the folders exist\n",
        "if os.path.exists(PROJECT_FOLDER):\n",
        "    print(\"Project Folder found:\", PROJECT_FOLDER)\n",
        "else:\n",
        "    print(\"Project Folder not found. Please check your path.\")\n",
        "\n",
        "if os.path.exists(TOOLS_FOLDER):\n",
        "    print(\"Tools Folder found:\", TOOLS_FOLDER)\n",
        "else:\n",
        "    print(\"Tools Folder not found. Please check your path.\")\n",
        "\n",
        "# Set executable permissions for all files and directories in both folders\n",
        "!chmod -R 755 \"$PROJECT_FOLDER\"\n",
        "!chmod -R 755 \"$TOOLS_FOLDER\"\n",
        "\n",
        "print(\"Permissions set for all contents in both folders.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Interactive Parameter Input\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Create input widgets for parameters\n",
        "anc1_widget = widgets.Text(value=\"Ggj\", description=\"Ancestor 1:\")\n",
        "anc2_widget = widgets.Text(value=\"Ggd\", description=\"Ancestor 2:\")\n",
        "target_widget = widgets.Text(value=\"Ggg\", description=\"Target Name:\")\n",
        "prefix_widget = widgets.Text(value=\"Ggg=GgjxGgd\", description=\"File Prefix:\")\n",
        "chr_set_widget = widgets.IntText(value=26, description=\"Chr Set:\")\n",
        "\n",
        "# Create a confirmation button\n",
        "confirm_button = widgets.Button(description=\"Confirm Parameters\", button_style='success')\n",
        "\n",
        "# Global configuration dictionary to store the parameters\n",
        "global_config = {}\n",
        "\n",
        "def confirm_params(b):\n",
        "    global global_config\n",
        "    global_config = {\n",
        "        \"anc1\": anc1_widget.value,\n",
        "        \"anc2\": anc2_widget.value,\n",
        "        \"target\": target_widget.value,\n",
        "        \"file_prefix\": prefix_widget.value,\n",
        "        \"chr_set\": chr_set_widget.value\n",
        "    }\n",
        "    clear_output()  # Clear the widget display after confirmation\n",
        "    print(\"Configuration confirmed:\")\n",
        "    for key, value in global_config.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "# Attach the callback to the button\n",
        "confirm_button.on_click(confirm_params)\n",
        "\n",
        "# Display the parameter input widgets\n",
        "param_widgets = widgets.VBox([anc1_widget, anc2_widget, target_widget, prefix_widget, chr_set_widget, confirm_button])\n",
        "display(param_widgets)"
      ],
      "metadata": {
        "id": "Vy-yl8j5CTrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Filtering Samples Using PLINK\n",
        "import os\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure PLINK executable exists\n",
        "plink_path = os.path.join(TOOLS_FOLDER, \"plink\")\n",
        "if not os.path.exists(plink_path):\n",
        "    raise FileNotFoundError(f\"❌ Error: PLINK not found at {plink_path}\")\n",
        "\n",
        "# Ensure global_config is loaded\n",
        "try:\n",
        "    if not global_config:\n",
        "        raise ValueError(\"Global configuration not found. Please run Step 2 to confirm parameters.\")\n",
        "except NameError:\n",
        "    raise ValueError(\"Global configuration not found. Please run Step 2 to confirm parameters.\")\n",
        "\n",
        "# Define file prefixes and paths\n",
        "input_prefix = os.path.join(PROJECT_FOLDER, global_config[\"file_prefix\"])  # Full path to input files\n",
        "output_prefix = os.path.join(PROJECT_FOLDER, global_config[\"file_prefix\"] + \"_filtered\")  # Output path\n",
        "\n",
        "# Check if required input files exist\n",
        "for ext in [\".bed\", \".bim\", \".fam\"]:\n",
        "    file_path = f\"{input_prefix}{ext}\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"❌ Error: Missing required file: {file_path}\")\n",
        "\n",
        "# Define filtering thresholds\n",
        "maf_threshold = 0.05    # Minor Allele Frequency threshold\n",
        "hwe_threshold = 1e-6    # Hardy-Weinberg Equilibrium threshold\n",
        "geno_threshold = 0.1    # SNP missingness threshold\n",
        "mind_threshold = 0.1    # Individual missingness threshold\n",
        "max_chr = global_config[\"chr_set\"]  # Number of chromosomes\n",
        "\n",
        "# Construct PLINK command\n",
        "plink_cmd = f\"\"\"\n",
        "{plink_path} --bfile {input_prefix} \\\n",
        "--chr-set {max_chr} \\\n",
        "--maf {maf_threshold} \\\n",
        "--hwe {hwe_threshold} \\\n",
        "--geno {geno_threshold} \\\n",
        "--mind {mind_threshold} \\\n",
        "--make-bed --out {output_prefix}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Running PLINK filtering with the following command:\")\n",
        "print(plink_cmd)\n",
        "\n",
        "# Execute PLINK command\n",
        "result = subprocess.run(plink_cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "# Print PLINK output and error messages\n",
        "print(\"PLINK Standard Output:\")\n",
        "print(result.stdout)\n",
        "\n",
        "if result.stderr:\n",
        "    print(\"PLINK Standard Error:\")\n",
        "    print(result.stderr)\n",
        "\n",
        "# Verify that the filtered .fam file was created and count the remaining samples\n",
        "filtered_fam_file = f\"{output_prefix}.fam\"\n",
        "if os.path.exists(filtered_fam_file):\n",
        "    fam_df = pd.read_csv(filtered_fam_file, sep=r\"\\s+\", header=None)\n",
        "    num_remaining_samples = fam_df.shape[0]\n",
        "    print(f\"✅ Number of samples remaining after filtering: {num_remaining_samples}\")\n",
        "\n",
        "    # Save the sample count for use in later steps\n",
        "    remaining_samples_file = os.path.join(PROJECT_FOLDER, \"remaining_samples.txt\")\n",
        "    with open(remaining_samples_file, \"w\") as f:\n",
        "        f.write(str(num_remaining_samples))\n",
        "    print(f\"✅ Remaining samples count saved at: {remaining_samples_file}\")\n",
        "else:\n",
        "    print(f\"❌ Error: Filtered .fam file ({filtered_fam_file}) not found. Check PLINK output or log file.\")"
      ],
      "metadata": {
        "id": "LhEjWeXKCcUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Admixture and PLINK - Following the workflow in try20.ipynb\n",
        "import os\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Helper function to run shell commands\n",
        "def run_command(command):\n",
        "    print(f\"\\nRunning command: {command}\")\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "    print(\"Command output:\")\n",
        "    print(result.stdout)\n",
        "    if result.returncode != 0:\n",
        "        print(\"Command error:\")\n",
        "        print(result.stderr)\n",
        "    return result\n",
        "\n",
        "# Use configuration from global_config\n",
        "try:\n",
        "    # global_config keys: file_prefix, target, anc1, anc2, chr_set\n",
        "    input_prefix = global_config[\"file_prefix\"]\n",
        "    filtered_prefix = input_prefix + \"_filtered\"\n",
        "    target_label = global_config[\"target\"]\n",
        "    anc1_label = global_config[\"anc1\"]\n",
        "    anc2_label = global_config[\"anc2\"]\n",
        "    max_chr = global_config[\"chr_set\"]\n",
        "except NameError:\n",
        "    raise ValueError(\"global_config not found. Please run the interactive parameter cell (Step 2) first.\")\n",
        "\n",
        "# Define paths to executables and file paths\n",
        "tools_dir = TOOLS_FOLDER\n",
        "plink_path = os.path.join(TOOLS_FOLDER, \"plink\")\n",
        "admixture_path = os.path.join(TOOLS_FOLDER, \"admixture\")\n",
        "\n",
        "# Build paths with PROJECT_FOLDER\n",
        "filtered_path = os.path.join(PROJECT_FOLDER, filtered_prefix)\n",
        "print(f\"Using filtered files: {filtered_path}\")\n",
        "\n",
        "# ------------------------------\n",
        "# Step: Run ADMIXTURE at K=3\n",
        "# ------------------------------\n",
        "K3 = 3\n",
        "admixture_cmd_K3 = f\"{admixture_path} {filtered_path}.bed {K3}\"\n",
        "print(f\"Running ADMIXTURE at K=3:\")\n",
        "run_command(admixture_cmd_K3)\n",
        "\n",
        "# Look for Q file in multiple locations\n",
        "q_file_K3 = f\"{filtered_path}.{K3}.Q\"\n",
        "if not os.path.exists(q_file_K3):\n",
        "    # Check if it's in the current directory\n",
        "    local_q_file = f\"{os.path.basename(filtered_prefix)}.{K3}.Q\"\n",
        "    if os.path.exists(local_q_file):\n",
        "        q_file_K3 = local_q_file\n",
        "        print(f\"Found K=3 Q file in current directory: {q_file_K3}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"ADMIXTURE Q file {q_file_K3} not found!\")\n",
        "\n",
        "# Load the K=3 Q file\n",
        "q3 = pd.read_csv(q_file_K3, delim_whitespace=True, header=None, dtype=float)\n",
        "q3.columns = [\"col0\", \"col1\", \"col2\"]\n",
        "\n",
        "# Load the .fam file\n",
        "fam_cols = [\"pop\", \"iid\", \"father\", \"mother\", \"sex\", \"phenotype\"]\n",
        "fam = pd.read_csv(f\"{filtered_path}.fam\", sep=r\"\\s+\", header=None, names=fam_cols)\n",
        "\n",
        "# Combine Q file with fam info (assuming row order matches)\n",
        "admix_K3 = pd.concat([fam[[\"pop\", \"iid\"]], q3], axis=1)\n",
        "\n",
        "# ------------------------------\n",
        "# Step: Assign Columns for K=3\n",
        "# ------------------------------\n",
        "# Identify \"pure\" ancestral individuals based on the pop labels\n",
        "pure_BI = admix_K3[admix_K3[\"pop\"] == anc1_label]\n",
        "pure_BT = admix_K3[admix_K3[\"pop\"] == anc2_label]\n",
        "\n",
        "if pure_BI.empty or pure_BT.empty:\n",
        "    print(\"Warning: One or both ancestral groups have no individuals. Using default column assignments.\")\n",
        "    BI_col = \"col0\"\n",
        "    target_col = \"col1\"\n",
        "    BT_col = \"col2\"\n",
        "else:\n",
        "    BI_avgs = pure_BI[[\"col0\", \"col1\", \"col2\"]].mean()\n",
        "    BT_avgs = pure_BT[[\"col0\", \"col1\", \"col2\"]].mean()\n",
        "    BI_col = BI_avgs.idxmax()  # assign column with highest average for anc1\n",
        "    BT_col = BT_avgs.idxmax()  # assign column with highest average for anc2\n",
        "    all_cols = {\"col0\", \"col1\", \"col2\"}\n",
        "    target_col = list(all_cols - {BI_col, BT_col})[0]\n",
        "\n",
        "print(f\"Assigned columns (K=3):\")\n",
        "print(f\"  {BI_col} -> {anc1_label}\")\n",
        "print(f\"  {target_col} -> {target_label}\")\n",
        "print(f\"  {BT_col} -> {anc2_label}\")\n",
        "\n",
        "# Plot K=3 admixture results\n",
        "plt.figure(figsize=(12, 5))\n",
        "sort_order = admix_K3.sort_values(\"pop\").index\n",
        "pop_colors = {'col0': 'red', 'col1': 'green', 'col2': 'blue'}\n",
        "\n",
        "# Create stacked bar chart\n",
        "bottom = np.zeros(len(admix_K3))\n",
        "for col in ['col0', 'col1', 'col2']:\n",
        "    values = admix_K3.loc[sort_order, col].values\n",
        "    plt.bar(range(len(values)), values, bottom=bottom, width=0.8,\n",
        "            color=pop_colors[col])\n",
        "    bottom += values\n",
        "\n",
        "# Add population labels\n",
        "pop_groups = admix_K3.loc[sort_order].groupby(\"pop\")\n",
        "tick_positions = []\n",
        "tick_labels = []\n",
        "start_pos = 0\n",
        "for pop, group in pop_groups:\n",
        "    group_size = len(group)\n",
        "    tick_positions.append(start_pos + group_size/2)\n",
        "    tick_labels.append(f\"{pop} (n={group_size})\")\n",
        "    start_pos += group_size\n",
        "    # Draw vertical line between populations\n",
        "    if start_pos < len(admix_K3):\n",
        "        plt.axvline(x=start_pos - 0.5, color='black', linestyle='--', alpha=0.3)\n",
        "\n",
        "plt.xticks(tick_positions, tick_labels, rotation=45, ha='right')\n",
        "plt.ylabel(\"Ancestry Proportion\")\n",
        "plt.title(\"ADMIXTURE Results (K=3)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(PROJECT_FOLDER, \"admixture_K3_results.png\"), dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Determine assignment for each individual (which Q column is highest)\n",
        "admix_K3[\"assigned\"] = admix_K3[[BI_col, target_col, BT_col]].idxmax(axis=1)\n",
        "\n",
        "# ------------------------------\n",
        "# Step: Filter Individuals Based on Composition Thresholds (K=3)\n",
        "# ------------------------------\n",
        "anc_threshold = 0.75    # For ancestral groups, require proportion >= 0.75\n",
        "target_threshold = 0.90 # For target, remove if proportion > 0.90\n",
        "\n",
        "# Filter ancestral BI group (assigned to BI)\n",
        "group_BI = admix_K3[admix_K3[\"assigned\"] == BI_col]\n",
        "removed_BI = group_BI[group_BI[BI_col] < anc_threshold]\n",
        "group_BI_filtered = group_BI[group_BI[BI_col] >= anc_threshold]\n",
        "\n",
        "# Filter ancestral BT group (assigned to BT)\n",
        "group_BT = admix_K3[admix_K3[\"assigned\"] == BT_col]\n",
        "removed_BT = group_BT[group_BT[BT_col] < anc_threshold]\n",
        "group_BT_filtered = group_BT[group_BT[BT_col] >= anc_threshold]\n",
        "\n",
        "# For the target population (e.g., PAH), remove individuals with high target proportion\n",
        "group_target = admix_K3[admix_K3[\"pop\"] == target_label]\n",
        "removed_target = group_target[group_target[target_col] > target_threshold]\n",
        "group_target_filtered = group_target[group_target[target_col] <= target_threshold]\n",
        "\n",
        "print(\"\\nFiltering results (K=3):\")\n",
        "print(f\"{anc1_label} group: {len(group_BI)} before, {len(removed_BI)} removed (<{anc_threshold}), {len(group_BI_filtered)} remaining.\")\n",
        "print(f\"{anc2_label} group: {len(group_BT)} before, {len(removed_BT)} removed (<{anc_threshold}), {len(group_BT_filtered)} remaining.\")\n",
        "print(f\"{target_label} group: {len(group_target)} before, {len(removed_target)} removed (>{target_threshold}), {len(group_target_filtered)} remaining.\")\n",
        "\n",
        "# Combine IDs of individuals to remove (from any group failing the thresholds)\n",
        "remove_ids = pd.concat([removed_BI[[\"pop\", \"iid\"]],\n",
        "                       removed_BT[[\"pop\", \"iid\"]],\n",
        "                       removed_target[[\"pop\", \"iid\"]]])\n",
        "remove_ids_file = os.path.join(PROJECT_FOLDER, \"remove_ids.txt\")\n",
        "remove_ids.to_csv(remove_ids_file, sep=\"\\t\", header=False, index=False)\n",
        "print(f\"\\nRemoval list saved to {remove_ids_file} (total individuals to remove: {remove_ids.shape[0]})\")\n",
        "\n",
        "# ------------------------------\n",
        "# Step: Create a New Filtered Dataset Using PLINK --remove\n",
        "# ------------------------------\n",
        "new_filtered_prefix = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_UM_filtered\")\n",
        "plink_remove_cmd = f\"{plink_path} --cow --bfile {filtered_path} --remove {remove_ids_file} --make-bed --out {new_filtered_prefix}\"\n",
        "print(\"\\nRunning PLINK --remove command:\")\n",
        "run_command(plink_remove_cmd)\n",
        "\n",
        "new_fam_file = f\"{new_filtered_prefix}.fam\"\n",
        "if os.path.exists(new_fam_file):\n",
        "    new_fam_df = pd.read_csv(new_fam_file, sep=r\"\\s+\", header=None)\n",
        "    new_sample_count = new_fam_df.shape[0]\n",
        "    print(f\"New sample count after removal: {new_sample_count}\")\n",
        "else:\n",
        "    print(\"New filtered .fam file not found. Check PLINK removal command.\")\n",
        "\n",
        "# ------------------------------\n",
        "# Step: Re-run ADMIXTURE at K=2 on the New Filtered Dataset\n",
        "# ------------------------------\n",
        "K2 = 2\n",
        "admixture_cmd_K2 = f\"{admixture_path} {new_filtered_prefix}.bed {K2}\"\n",
        "print(\"\\nRunning ADMIXTURE at K=2 with command:\")\n",
        "print(admixture_cmd_K2)\n",
        "run_command(admixture_cmd_K2)\n",
        "\n",
        "q_file_K2 = f\"{new_filtered_prefix}.{K2}.Q\"\n",
        "if not os.path.exists(q_file_K2):\n",
        "    # Check if it's in the current directory with a different path\n",
        "    local_q_file = f\"{os.path.basename(new_filtered_prefix)}.{K2}.Q\"\n",
        "    if os.path.exists(local_q_file):\n",
        "        q_file_K2 = local_q_file\n",
        "        print(f\"Found K=2 Q file in current directory: {q_file_K2}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"ADMIXTURE Q file {q_file_K2} not found after removal!\")\n",
        "\n",
        "# Load the K=2 Q file\n",
        "q2 = pd.read_csv(q_file_K2, delim_whitespace=True, header=None, dtype=float)\n",
        "q2.columns = [\"col0\", \"col1\"]\n",
        "\n",
        "# Load the new .fam file with population info\n",
        "new_fam = pd.read_csv(new_fam_file, delim_whitespace=True, header=None, names=fam_cols)\n",
        "\n",
        "# Combine Q file with fam info for K=2\n",
        "q2_full = pd.concat([new_fam[[\"pop\", \"iid\"]], q2], axis=1)\n",
        "\n",
        "# Determine column assignments for K=2 using pure ancestral individuals\n",
        "pure_BI_new = q2_full[q2_full[\"pop\"] == anc1_label]\n",
        "if not pure_BI_new.empty:\n",
        "    BI_avgs_new = pure_BI_new[[\"col0\", \"col1\"]].mean()\n",
        "    BI_col_K2 = BI_avgs_new.idxmax()  # assign column with highest average among BI individuals as BI\n",
        "else:\n",
        "    print(\"Warning: No individuals for ancestral group\", anc1_label, \"in new filtered file. Defaulting to col0.\")\n",
        "    BI_col_K2 = \"col0\"\n",
        "all2 = {\"col0\", \"col1\"}\n",
        "BT_col_K2 = list(all2 - {BI_col_K2})[0]\n",
        "\n",
        "# Rename Q file columns to ancestral labels\n",
        "q2_full = q2_full.rename(columns={BI_col_K2: anc1_label, BT_col_K2: anc2_label})\n",
        "\n",
        "# Compute average ancestry for target individuals in K=2\n",
        "target_new = q2_full[q2_full[\"pop\"] == target_label]\n",
        "target_anc1_avg_new = target_new[anc1_label].mean() if not target_new.empty else np.nan\n",
        "target_anc2_avg_new = target_new[anc2_label].mean() if not target_new.empty else np.nan\n",
        "\n",
        "print(\"\\nADMIXTURE (K=2) average ancestry in target population:\")\n",
        "print(f\"  {anc1_label}: {target_anc1_avg_new:.4f}\")\n",
        "print(f\"  {anc2_label}: {target_anc2_avg_new:.4f}\")\n",
        "\n",
        "# ------------------------------\n",
        "# Updated ADMIXTURE (K=2) Bar Plot with Grouped x-axis\n",
        "# ------------------------------\n",
        "# Sort samples by population to group them\n",
        "q2_full_sorted = q2_full.sort_values(\"pop\").reset_index(drop=True)\n",
        "\n",
        "# Determine x positions with gaps between populations\n",
        "pop_groups = q2_full_sorted.groupby(\"pop\")\n",
        "x_positions = []         # x position for each sample\n",
        "tick_positions = {}      # x position for labeling each population group\n",
        "current_pos = 0\n",
        "gap = 1  # gap between groups (in x coordinate units)\n",
        "for pop, group in pop_groups:\n",
        "    group_length = group.shape[0]\n",
        "    positions = list(range(current_pos, current_pos + group_length))\n",
        "    x_positions.extend(positions)\n",
        "    tick_positions[pop] = current_pos + (group_length - 1) / 2\n",
        "    current_pos += group_length + gap\n",
        "\n",
        "x_positions = np.array(x_positions)\n",
        "bottom_vals = np.zeros(len(q2_full_sorted))\n",
        "colors = [\"red\", \"blue\"]\n",
        "\n",
        "plt.figure(figsize=(12, 5), dpi=300)\n",
        "for i, col in enumerate([anc1_label, anc2_label]):\n",
        "    plt.bar(x_positions, q2_full_sorted[col], bottom=bottom_vals, color=colors[i], width=0.8)\n",
        "    bottom_vals += q2_full_sorted[col].values\n",
        "\n",
        "plt.ylabel(\"Ancestry Proportion\")\n",
        "plt.xticks(list(tick_positions.values()), list(tick_positions.keys()), rotation=45)\n",
        "plt.xlim(min(x_positions)-0.5, max(x_positions)+0.5)\n",
        "plt.title(\"ADMIXTURE Results (K=2)\")\n",
        "plt.tight_layout()\n",
        "admixture_plot_K2 = os.path.join(PROJECT_FOLDER, \"admixture_barplot_filtered_K2.png\")\n",
        "plt.savefig(admixture_plot_K2, dpi=300)\n",
        "plt.show()\n",
        "print(f\"ADMIXTURE K=2 plot saved as {admixture_plot_K2}\")\n",
        "\n",
        "# ------------------------------\n",
        "# Run PCA on the New Filtered Dataset\n",
        "# ------------------------------\n",
        "pca_out_path = os.path.join(PROJECT_FOLDER, \"pca_out_filtered_K2\")\n",
        "pca_cmd_K2 = f\"{plink_path} --cow --nonfounders --bfile {new_filtered_prefix} --pca 2 --out {pca_out_path}\"\n",
        "print(\"\\nRunning PCA on new filtered dataset (K=2):\")\n",
        "run_command(pca_cmd_K2)\n",
        "\n",
        "pca_file_K2 = f\"{pca_out_path}.eigenvec\"\n",
        "if os.path.exists(pca_file_K2):\n",
        "    pca_df_K2 = pd.read_csv(pca_file_K2, delim_whitespace=True, header=None)\n",
        "    pca_df_K2.columns = [\"FID\", \"IID\", \"PC1\", \"PC2\"]\n",
        "    pca_merged_K2 = pd.merge(pca_df_K2, new_fam[[\"pop\", \"iid\"]], left_on=\"IID\", right_on=\"iid\")\n",
        "    plt.figure(figsize=(8,6), dpi=300)\n",
        "    unique_pops = pca_merged_K2[\"pop\"].unique()\n",
        "    pop_colors = {pop: col for pop, col in zip(unique_pops, [\"red\", \"blue\", \"green\", \"orange\", \"purple\"])}\n",
        "    for pop in unique_pops:\n",
        "        subset = pca_merged_K2[pca_merged_K2[\"pop\"] == pop]\n",
        "        plt.scatter(subset[\"PC1\"], subset[\"PC2\"], label=pop, color=pop_colors.get(pop, \"grey\"), s=50, alpha=0.7)\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.legend(title=\"Population\", loc=\"best\")\n",
        "    plt.title(\"PCA of Filtered Dataset (K=2)\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    pca_plot_K2 = os.path.join(PROJECT_FOLDER, \"pca_plot_filtered_K2.png\")\n",
        "    plt.savefig(pca_plot_K2, dpi=300)\n",
        "    plt.show()\n",
        "    print(f\"PCA plot (K=2) saved as {pca_plot_K2}\")\n",
        "else:\n",
        "    print(\"PCA eigenvec file (K=2) not found.\")\n",
        "\n",
        "# ------------------------------\n",
        "# Step: Save Summary Information to a LaTeX File\n",
        "# ------------------------------\n",
        "original_target_count = len(fam[fam[\"pop\"] == target_label])\n",
        "filtered_target_count = len(target_new) if 'target_new' in locals() and not target_new.empty else np.nan\n",
        "\n",
        "latex_file = os.path.join(PROJECT_FOLDER, \"ancestry_summary.tex\")\n",
        "with open(latex_file, \"w\") as f:\n",
        "    f.write(r\"\\documentclass{article}\" + \"\\n\")\n",
        "    f.write(r\"\\begin{document}\" + \"\\n\\n\")\n",
        "    f.write(r\"\\section*{ADMIXTURE and PCA Summary}\" + \"\\n\")\n",
        "    f.write(\"Number of target samples before filtering: \" + str(original_target_count) + r\"\\\\\")\n",
        "    f.write(\"\\nNumber of target samples after filtering (K=2): \" + str(filtered_target_count) + r\"\\\\\")\n",
        "    f.write(\"\\n\\\\vspace{0.5cm}\" + \"\\n\")\n",
        "    f.write(r\"\\begin{tabular}{lcc}\" + \"\\n\")\n",
        "    f.write(r\"\\hline\" + \"\\n\")\n",
        "    f.write(\"Population & Avg. Ancestry (K=3) & Avg. Ancestry (K=2) \\\\\\\\\" + \"\\n\")\n",
        "    f.write(r\"\\hline\" + \"\\n\")\n",
        "    if not group_target_filtered.empty:\n",
        "        target_anc1_avg_K3 = group_target_filtered[BI_col].mean()\n",
        "        target_anc2_avg_K3 = group_target_filtered[target_col].mean()\n",
        "    else:\n",
        "        target_anc1_avg_K3, target_anc2_avg_K3 = np.nan, np.nan\n",
        "    f.write(f\"{anc1_label} & {target_anc1_avg_K3:.4f} & {target_anc1_avg_new:.4f} \\\\\\\\\" + \"\\n\")\n",
        "    f.write(f\"{anc2_label} & {target_anc2_avg_K3:.4f} & {target_anc2_avg_new:.4f} \\\\\\\\\" + \"\\n\")\n",
        "    f.write(r\"\\hline\" + \"\\n\")\n",
        "    f.write(r\"\\end{tabular}\" + \"\\n\\n\")\n",
        "    f.write(r\"\\end{document}\" + \"\\n\")\n",
        "\n",
        "print(f\"\\nLaTeX summary file '{latex_file}' created.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4OVDKjY6Cl4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Calculate Ancestral Allele Frequencies\n",
        "import os\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Use configuration from global_config\n",
        "try:\n",
        "    # Retrieve configuration from global_config (set in Step 2)\n",
        "    input_prefix    = global_config[\"file_prefix\"]\n",
        "    filtered_prefix = input_prefix + \"_filtered\"\n",
        "    target_label    = global_config[\"target\"]\n",
        "    anc_pop1_label  = global_config[\"anc1\"]\n",
        "    anc_pop2_label  = global_config[\"anc2\"]\n",
        "    max_chr         = global_config[\"chr_set\"]\n",
        "except NameError:\n",
        "    raise ValueError(\"global_config not found. Please run the interactive parameter cell (Step 2) first.\")\n",
        "\n",
        "# Define the path to PLINK executable\n",
        "tools_dir = TOOLS_FOLDER\n",
        "plink_path = os.path.join(TOOLS_FOLDER, \"plink\")\n",
        "\n",
        "# Build proper paths for input and output files\n",
        "filtered_path = os.path.join(PROJECT_FOLDER, filtered_prefix)\n",
        "output_freq_prefix = os.path.join(PROJECT_FOLDER, \"Afreq_LocaBreed\")\n",
        "\n",
        "############################################\n",
        "# Step: Calculate Ancestral Allele Frequencies using PLINK\n",
        "############################################\n",
        "freq_cmd = f'{plink_path} --cow --nonfounders --bfile {filtered_path} --freq --family --out {output_freq_prefix}'\n",
        "print(\"Running PLINK for allele frequency calculation:\")\n",
        "subprocess.run(freq_cmd, shell=True, check=True)\n",
        "print(\"Allele frequency calculation completed.\")\n",
        "\n",
        "############################################\n",
        "# Step: Process Allele Frequencies File\n",
        "############################################\n",
        "freq_file = f\"{output_freq_prefix}.frq.strat\"\n",
        "if not os.path.exists(freq_file):\n",
        "    raise FileNotFoundError(f\"Frequency file {freq_file} not found!\")\n",
        "\n",
        "afreq = pd.read_csv(freq_file, delim_whitespace=True)\n",
        "# Expected columns in .frq.strat (e.g.): CHR, SNP, A1, A2, MAF, NCHROBS, CLST\n",
        "# Compute allele frequency for allele A: by default use MAF\n",
        "afreq[\"AlleleA\"] = afreq[\"MAF\"]\n",
        "# If the alternate allele (A2) is coded as \"2\", then flip the frequency\n",
        "afreq.loc[afreq[\"A2\"] == \"2\", \"AlleleA\"] = 1 - afreq.loc[afreq[\"A2\"] == \"2\", \"MAF\"]\n",
        "print(\"Processed allele frequencies.\")\n",
        "\n",
        "# Display population distribution in frequency file\n",
        "print(\"\\nPopulation distribution in frequency file:\")\n",
        "pop_counts = afreq[\"CLST\"].value_counts()\n",
        "for pop, count in pop_counts.items():\n",
        "    print(f\"  {pop}: {count} records\")\n",
        "\n",
        "############################################\n",
        "# Step: Write Ancestral Allele Frequency Files per Chromosome\n",
        "############################################\n",
        "# Subset frequency file by ancestral population using the CLST column\n",
        "afreq_anc1 = afreq[afreq[\"CLST\"] == anc_pop1_label]\n",
        "afreq_anc2 = afreq[afreq[\"CLST\"] == anc_pop2_label]\n",
        "\n",
        "print(f\"\\nWriting allele frequency files per chromosome:\")\n",
        "for i in range(1, max_chr + 1):\n",
        "    out_file_anc1 = os.path.join(PROJECT_FOLDER, f\"purebred_{anc_pop1_label}_CHR{i}.prob\")\n",
        "    out_file_anc2 = os.path.join(PROJECT_FOLDER, f\"purebred_{anc_pop2_label}_CHR{i}.prob\")\n",
        "\n",
        "    # Write allele frequencies (AlleleA) for SNPs on chromosome i\n",
        "    data_anc1 = afreq_anc1.loc[afreq_anc1[\"CHR\"] == i, \"AlleleA\"]\n",
        "    data_anc2 = afreq_anc2.loc[afreq_anc2[\"CHR\"] == i, \"AlleleA\"]\n",
        "\n",
        "    data_anc1.to_csv(out_file_anc1, sep=\"\\t\", header=False, index=False, quoting=3)\n",
        "    data_anc2.to_csv(out_file_anc2, sep=\"\\t\", header=False, index=False, quoting=3)\n",
        "    print(f\"  Chromosome {i}:\")\n",
        "    print(f\"    {os.path.basename(out_file_anc1)} ({len(data_anc1)} SNPs)\")\n",
        "    print(f\"    {os.path.basename(out_file_anc2)} ({len(data_anc2)} SNPs)\")\n",
        "\n",
        "############################################\n",
        "# Step: Create Keep File for Target Individuals\n",
        "############################################\n",
        "# Read the filtered .fam file\n",
        "fam_file = f\"{filtered_path}.fam\"\n",
        "fam_cols = [\"pop\", \"iid\", \"father\", \"mother\", \"sex\", \"phenotype\"]\n",
        "fam_df = pd.read_csv(fam_file, sep=r\"\\s+\", header=None, names=fam_cols)\n",
        "\n",
        "# Select individuals whose population equals target_label\n",
        "target_keep = fam_df[fam_df[\"pop\"] == target_label][[\"pop\", \"iid\"]]\n",
        "target_keep_file = os.path.join(PROJECT_FOLDER, f\"{target_label}_crossbreds.keep\")\n",
        "target_keep.to_csv(target_keep_file, sep=\"\\t\", header=False, index=False)\n",
        "print(f\"\\nCreated keep file '{os.path.basename(target_keep_file)}' with {target_keep.shape[0]} individuals.\")\n",
        "\n",
        "############################################\n",
        "# Calculate ancestral sample sizes from the filtered .fam file\n",
        "############################################\n",
        "pure_anc1_count = fam_df[fam_df[\"pop\"] == anc_pop1_label].shape[0]\n",
        "pure_anc2_count = fam_df[fam_df[\"pop\"] == anc_pop2_label].shape[0]\n",
        "print(f\"Ancestral sample sizes: {anc_pop1_label} = {pure_anc1_count}, {anc_pop2_label} = {pure_anc2_count}\")\n",
        "\n",
        "############################################\n",
        "# Step: Extract Each Chromosome from the Filtered Dataset\n",
        "############################################\n",
        "print(\"\\nExtracting chromosome-specific datasets:\")\n",
        "for i in range(1, max_chr + 1):\n",
        "    chr_out = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{i}\")\n",
        "    cmd = f'{plink_path} --cow --nonfounders --bfile {filtered_path} --chr {i} --make-bed --out {chr_out}'\n",
        "    print(f\"  Chromosome {i}... \", end=\"\", flush=True)\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        print(\"success\")\n",
        "    else:\n",
        "        print(\"failed\")\n",
        "        print(f\"  Error: {result.stderr}\")\n",
        "\n",
        "############################################\n",
        "# Step: Create Recode Allele Input Files for Each Chromosome\n",
        "############################################\n",
        "print(\"\\nCreating recode files for each chromosome:\")\n",
        "for i in range(1, max_chr + 1):\n",
        "    bim_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{i}.bim\")\n",
        "    recode_file = os.path.join(PROJECT_FOLDER, f\"recodeAB_CHR{i}.txt\")\n",
        "\n",
        "    if os.path.exists(bim_file):\n",
        "        bim_df = pd.read_csv(bim_file, delim_whitespace=True, header=None)\n",
        "        # In the PLINK .bim file, column 2 (index 1) is the SNP ID.\n",
        "        # Create a DataFrame with SNP ID and a constant \"2\"\n",
        "        recode_df = pd.DataFrame({\n",
        "            0: bim_df.iloc[:, 1],\n",
        "            1: [\"2\"] * bim_df.shape[0]\n",
        "        })\n",
        "        recode_df.to_csv(recode_file, sep=\"\\t\", header=False, index=False, quoting=3)\n",
        "        print(f\"  Created recode file for chromosome {i} with {bim_df.shape[0]} SNPs\")\n",
        "    else:\n",
        "        print(f\"  Skipping chromosome {i} - BIM file not found\")\n",
        "\n",
        "############################################\n",
        "# Step: Recode Genotypes for Each Chromosome Using PLINK\n",
        "############################################\n",
        "print(\"\\nRecoding genotypes for each chromosome:\")\n",
        "for i in range(1, max_chr + 1):\n",
        "    chr_in = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{i}\")\n",
        "    recode_file = os.path.join(PROJECT_FOLDER, f\"recodeAB_CHR{i}.txt\")\n",
        "\n",
        "    if os.path.exists(f\"{chr_in}.bed\") and os.path.exists(recode_file):\n",
        "        cmd = f'{plink_path} --cow --nonfounders --bfile {chr_in} --keep {target_keep_file} --reference-allele {recode_file} --recode A --out {chr_in}'\n",
        "        print(f\"  Chromosome {i}... \", end=\"\", flush=True)\n",
        "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"success\")\n",
        "            # Check if the output file was created\n",
        "            if os.path.exists(f\"{chr_in}.raw\"):\n",
        "                print(f\"    Created {os.path.basename(chr_in)}.raw\")\n",
        "            else:\n",
        "                print(f\"    Warning: {os.path.basename(chr_in)}.raw not found\")\n",
        "        else:\n",
        "            print(\"failed\")\n",
        "            print(f\"    Error: {result.stderr}\")\n",
        "    else:\n",
        "        print(f\"  Skipping chromosome {i} - required input files not found\")\n",
        "\n",
        "print(\"\\nCompleted all steps for calculating ancestral allele frequencies.\")\n",
        "print(f\"Files are saved in: {PROJECT_FOLDER}\")\n",
        "\n",
        "# Print summary of the files created\n",
        "print(\"\\nSummary of key files created:\")\n",
        "print(f\"1. Allele frequency file: {os.path.basename(output_freq_prefix)}.frq.strat\")\n",
        "print(f\"2. Ancestral allele probability files: purebred_{anc_pop1_label}_CHR*.prob, purebred_{anc_pop2_label}_CHR*.prob\")\n",
        "print(f\"3. Target individuals keep file: {os.path.basename(target_keep_file)}\")\n",
        "print(f\"4. Chromosome-specific datasets: {os.path.basename(filtered_prefix)}_CHR*.bed/.bim/.fam\")\n",
        "print(f\"5. Recode files: recodeAB_CHR*.txt\")\n",
        "print(f\"6. Recoded genotypes: {os.path.basename(filtered_prefix)}_CHR*.raw\")"
      ],
      "metadata": {
        "id": "id8DBkRTGXY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Prepare LAMP Input Files\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Use configuration from global_config\n",
        "try:\n",
        "    # Retrieve configuration from global_config\n",
        "    input_prefix    = global_config[\"file_prefix\"]\n",
        "    filtered_prefix = input_prefix + \"_filtered\"\n",
        "    target_label    = global_config[\"target\"]\n",
        "    anc_pop1_label  = global_config[\"anc1\"]\n",
        "    anc_pop2_label  = global_config[\"anc2\"]\n",
        "    max_chr         = global_config[\"chr_set\"]\n",
        "except NameError:\n",
        "    raise ValueError(\"global_config not found. Please run the interactive parameter cell (Step 2) first.\")\n",
        "\n",
        "# Determine the final filtered prefix after admixture-based filtering\n",
        "final_filtered_prefix = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_UM_filtered\")\n",
        "\n",
        "# Extract alpha values from ancestry_summary.tex\n",
        "def extract_alpha_from_tex(tex_file, population_labels):\n",
        "    try:\n",
        "        with open(tex_file, 'r') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        pattern = re.compile(rf\"{population_labels[0]}\\\\s+&\\\\s+[\\\\d.]+\\\\s+&\\\\s+([\\\\d.]+).*?{population_labels[1]}\\\\s+&\\\\s+[\\\\d.]+\\\\s+&\\\\s+([\\\\d.]+)\", re.DOTALL)\n",
        "        match = pattern.search(content)\n",
        "\n",
        "        if match:\n",
        "            alpha1, alpha2 = float(match.group(1)), float(match.group(2))\n",
        "            total = alpha1 + alpha2\n",
        "            alpha1 /= total\n",
        "            alpha2 /= total\n",
        "            return alpha1, alpha2\n",
        "        else:\n",
        "            # If we can't find the exact pattern, look for any floating point numbers after the labels\n",
        "            alt_pattern1 = re.compile(rf\"{population_labels[0]}.*?([0-9.]+)\")\n",
        "            alt_pattern2 = re.compile(rf\"{population_labels[1]}.*?([0-9.]+)\")\n",
        "\n",
        "            match1 = alt_pattern1.search(content)\n",
        "            match2 = alt_pattern2.search(content)\n",
        "\n",
        "            if match1 and match2:\n",
        "                alpha1, alpha2 = float(match1.group(1)), float(match2.group(1))\n",
        "                total = alpha1 + alpha2\n",
        "                alpha1 /= total\n",
        "                alpha2 /= total\n",
        "                return alpha1, alpha2\n",
        "            else:\n",
        "                print(f\"Warning: Could not extract ancestry proportions from {tex_file}\")\n",
        "                # Return default values\n",
        "                return 0.5, 0.5\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {tex_file}: {str(e)}\")\n",
        "        return 0.5, 0.5\n",
        "\n",
        "# Fetch ancestral sample sizes from the filtered .fam file\n",
        "def fetch_ancestral_sizes(fam_file, anc_labels):\n",
        "    try:\n",
        "        fam_df = pd.read_csv(fam_file, delim_whitespace=True, header=None)\n",
        "        anc_size1 = fam_df[fam_df[0] == anc_labels[0]].shape[0]\n",
        "        anc_size2 = fam_df[fam_df[0] == anc_labels[1]].shape[0]\n",
        "        return anc_size1, anc_size2\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching ancestral sizes from {fam_file}: {str(e)}\")\n",
        "        return 10, 10  # Default values if file can't be read\n",
        "\n",
        "# Function to prepare LAMP input files\n",
        "def prepare_lamp_inputs(chrom, base_prefix, alpha1, alpha2, anc_size1, anc_size2, anc_labels):\n",
        "    prefix = f\"{base_prefix}_filtered\"\n",
        "    raw_file = os.path.join(PROJECT_FOLDER, f\"{prefix}_CHR{chrom}.raw\")\n",
        "    bim_file = os.path.join(PROJECT_FOLDER, f\"{prefix}_CHR{chrom}.bim\")\n",
        "    lampgeno_file = os.path.join(PROJECT_FOLDER, f\"{prefix}_CHR{chrom}_LAMPGENO.txt\")\n",
        "    lampmap_file = os.path.join(PROJECT_FOLDER, f\"{prefix}_CHR{chrom}_LAMPMAP.txt\")\n",
        "    config_file = os.path.join(PROJECT_FOLDER, f\"{prefix}_CHR{chrom}_configfile.txt\")\n",
        "\n",
        "    # LAMPGENO file - extract genotype data from raw file\n",
        "    if os.path.exists(raw_file):\n",
        "        try:\n",
        "            with open(raw_file, 'r') as infile:\n",
        "                lines = infile.readlines()\n",
        "\n",
        "            if len(lines) > 1:\n",
        "                with open(lampgeno_file, 'w') as outfile:\n",
        "                    for line in lines[1:]:  # Skip header\n",
        "                        parts = line.strip().split()\n",
        "                        geno = parts[6:]  # Skip FID,IID,PAT,MAT,SEX,PHENOTYPE columns\n",
        "                        # Replace missing values with -1\n",
        "                        geno = [\"-1\" if x.upper() == \"NA\" else x for x in geno]\n",
        "                        outfile.write(\" \".join(geno) + \"\\n\")\n",
        "                print(f\"[CHR{chrom}] Created LAMPGENO: {os.path.basename(lampgeno_file)}\")\n",
        "            else:\n",
        "                print(f\"[CHR{chrom}] {raw_file} has only a header row or is empty. No data to process.\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"[CHR{chrom}] Error processing raw file: {str(e)}\")\n",
        "            return False\n",
        "    else:\n",
        "        print(f\"[CHR{chrom}] Raw file not found: {raw_file}\")\n",
        "        return False\n",
        "\n",
        "    # LAMPMAP file - extract position data from bim file\n",
        "    if os.path.exists(bim_file):\n",
        "        try:\n",
        "            with open(bim_file, 'r') as infile, open(lampmap_file, 'w') as outfile:\n",
        "                for line in infile:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) >= 4:\n",
        "                        # Column 4 (index 3) in BIM file contains the position\n",
        "                        outfile.write(parts[3] + \"\\n\")\n",
        "            print(f\"[CHR{chrom}] Created LAMPMAP: {os.path.basename(lampmap_file)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[CHR{chrom}] Error processing BIM file: {str(e)}\")\n",
        "            return False\n",
        "    else:\n",
        "        print(f\"[CHR{chrom}] BIM file not found: {bim_file}\")\n",
        "        return False\n",
        "\n",
        "    # Create LAMP configuration file\n",
        "    prob_file1 = os.path.join(PROJECT_FOLDER, f\"purebred_{anc_labels[0]}_CHR{chrom}.prob\")\n",
        "    prob_file2 = os.path.join(PROJECT_FOLDER, f\"purebred_{anc_labels[1]}_CHR{chrom}.prob\")\n",
        "\n",
        "    config_content = (\n",
        "        \"populations=2\\n\"\n",
        "        f\"pfile={prob_file1},{prob_file2}\\n\"\n",
        "        f\"ancestralsamplesize={anc_size1},{anc_size2}\\n\"\n",
        "        f\"genofile={lampgeno_file}\\n\"\n",
        "        f\"posfile={lampmap_file}\\n\"\n",
        "        f\"outputancestryfile={os.path.join(PROJECT_FOLDER, f'{prefix}_CHR{chrom}_results.txt')}\\n\"\n",
        "        \"offset=0.2\\n\"\n",
        "        \"recombrate=1e-8\\n\"\n",
        "        \"generations=02\\n\"\n",
        "        f\"alpha={alpha1:.4f},{alpha2:.4f}\\n\"\n",
        "        \"ldcutoff=1\\n\"\n",
        "    )\n",
        "\n",
        "    with open(config_file, 'w') as f:\n",
        "        f.write(config_content)\n",
        "    print(f\"[CHR{chrom}] Created LAMP config: {os.path.basename(config_file)}\")\n",
        "    return True\n",
        "\n",
        "# Main execution\n",
        "print(\"Step 6: Preparing LAMP Input Files\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Fetch alpha values from ancestry_summary.tex\n",
        "tex_file_path = os.path.join(PROJECT_FOLDER, \"ancestry_summary.tex\")\n",
        "alpha1, alpha2 = extract_alpha_from_tex(tex_file_path, [anc_pop1_label, anc_pop2_label])\n",
        "print(f\"Ancestry proportions extracted: {anc_pop1_label}={alpha1:.4f}, {anc_pop2_label}={alpha2:.4f}\")\n",
        "\n",
        "# Fetch ancestral sample sizes\n",
        "fam_file_path = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_UM_filtered.fam\")\n",
        "if not os.path.exists(fam_file_path):\n",
        "    fam_file_path = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}.fam\")\n",
        "    print(f\"Warning: Using original filtered .fam file: {os.path.basename(fam_file_path)}\")\n",
        "\n",
        "anc_size1, anc_size2 = fetch_ancestral_sizes(fam_file_path, [anc_pop1_label, anc_pop2_label])\n",
        "print(f\"Ancestral sample sizes: {anc_pop1_label}={anc_size1}, {anc_pop2_label}={anc_size2}\")\n",
        "\n",
        "# Check if required files exist\n",
        "print(\"\\nChecking for required input files:\")\n",
        "files_missing = False\n",
        "for i in range(1, max_chr + 1):\n",
        "    raw_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{i}.raw\")\n",
        "    bim_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{i}.bim\")\n",
        "    prob_file1 = os.path.join(PROJECT_FOLDER, f\"purebred_{anc_pop1_label}_CHR{i}.prob\")\n",
        "    prob_file2 = os.path.join(PROJECT_FOLDER, f\"purebred_{anc_pop2_label}_CHR{i}.prob\")\n",
        "\n",
        "    chr_status = []\n",
        "    for f, desc in [\n",
        "        (raw_file, \"raw file\"),\n",
        "        (bim_file, \"bim file\"),\n",
        "        (prob_file1, f\"{anc_pop1_label} prob file\"),\n",
        "        (prob_file2, f\"{anc_pop2_label} prob file\")\n",
        "    ]:\n",
        "        if os.path.exists(f):\n",
        "            chr_status.append(f\"✓ {desc}\")\n",
        "        else:\n",
        "            chr_status.append(f\"✗ {desc}\")\n",
        "            files_missing = True\n",
        "\n",
        "    print(f\"  Chromosome {i}: \" + \", \".join(chr_status))\n",
        "\n",
        "if files_missing:\n",
        "    print(\"\\nWarning: Some required files are missing. LAMP preparation may be incomplete.\")\n",
        "else:\n",
        "    print(\"\\nAll required files for LAMP analysis are present.\")\n",
        "\n",
        "# Loop over chromosomes and prepare LAMP input files\n",
        "print(\"\\nPreparing LAMP input files for each chromosome:\")\n",
        "prepared_count = 0\n",
        "for i in range(1, max_chr + 1):\n",
        "    print(f\"\\n=== Preparing LAMP input files for Chromosome {i} ===\")\n",
        "    success = prepare_lamp_inputs(i, input_prefix, alpha1, alpha2, anc_size1, anc_size2, [anc_pop1_label, anc_pop2_label])\n",
        "    if success:\n",
        "        prepared_count += 1\n",
        "\n",
        "print(f\"\\nSuccessfully prepared LAMP input files for {prepared_count} out of {max_chr} chromosomes\")\n",
        "\n",
        "# Print summary of configuration for LAMP\n",
        "print(\"\\nLAMP Configuration Summary:\")\n",
        "print(f\"  Populations: 2 ({anc_pop1_label}, {anc_pop2_label})\")\n",
        "print(f\"  Ancestry proportions: {anc_pop1_label}={alpha1:.4f}, {anc_pop2_label}={alpha2:.4f}\")\n",
        "print(f\"  Ancestral sample sizes: {anc_pop1_label}={anc_size1}, {anc_pop2_label}={anc_size2}\")\n",
        "print(\"  Other parameters:\")\n",
        "print(\"    - offset: 0.2\")\n",
        "print(\"    - recombination rate: 1e-8\")\n",
        "print(\"    - generations: 2\")\n",
        "print(\"    - LD cutoff: 1\")\n",
        "\n",
        "# Create a shell script to run LAMP for all chromosomes\n",
        "lamp_script_file = os.path.join(PROJECT_FOLDER, \"run_lamp.sh\")\n",
        "with open(lamp_script_file, 'w') as f:\n",
        "    f.write(\"#!/bin/bash\\n\\n\")\n",
        "    f.write(f\"LAMP_PATH=\\\"{os.path.join(TOOLS_FOLDER, 'lamp')}\\\"\\n\\n\")\n",
        "    f.write(\"# Run LAMP for each chromosome\\n\")\n",
        "    for i in range(1, max_chr + 1):\n",
        "        config_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{i}_configfile.txt\")\n",
        "        f.write(f\"echo \\\"Processing chromosome {i}...\\\"\\n\")\n",
        "        f.write(f\"\\\"$LAMP_PATH\\\" {config_file}\\n\")\n",
        "        f.write(\"echo \\\"Done with chromosome ${i}\\\"\\n\\n\")\n",
        "\n",
        "# Make the script executable\n",
        "os.chmod(lamp_script_file, 0o755)\n",
        "print(f\"\\nCreated a shell script to run LAMP: {os.path.basename(lamp_script_file)}\")\n",
        "print(\"You can run this script to execute LAMP on all chromosomes\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nap4QDMTGmke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Run LAMP Analysis for Each Chromosome\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Use configuration from global_config\n",
        "try:\n",
        "    # Retrieve configuration from global_config\n",
        "    input_prefix    = global_config[\"file_prefix\"]\n",
        "    filtered_prefix = input_prefix + \"_filtered\"\n",
        "    target_label    = global_config[\"target\"]\n",
        "    anc_pop1_label  = global_config[\"anc1\"]\n",
        "    anc_pop2_label  = global_config[\"anc2\"]\n",
        "    max_chr         = global_config[\"chr_set\"]\n",
        "except NameError:\n",
        "    raise ValueError(\"global_config not found. Please run the interactive parameter cell (Step 2) first.\")\n",
        "\n",
        "# Define the LAMP executable path\n",
        "lamp_exe = os.path.join(TOOLS_FOLDER, \"lamp\")\n",
        "\n",
        "# Check if LAMP executable exists\n",
        "if not os.path.exists(lamp_exe):\n",
        "    print(f\"Warning: LAMP executable not found at {lamp_exe}\")\n",
        "    lamp_exe = \"lamp\"  # Try using the command directly\n",
        "    print(f\"Will try using 'lamp' command directly. Ensure it's in your PATH.\")\n",
        "\n",
        "# Function to run LAMP for a single chromosome\n",
        "def run_lamp_for_chromosome(chrom):\n",
        "    config_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{chrom}_configfile.txt\")\n",
        "    results_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{chrom}_results.txt\")\n",
        "\n",
        "    # Check if config file exists\n",
        "    if not os.path.exists(config_file):\n",
        "        print(f\"Error: Config file not found for chromosome {chrom}: {config_file}\")\n",
        "        return False\n",
        "\n",
        "    # Check if results file already exists\n",
        "    if os.path.exists(results_file):\n",
        "        print(f\"Results file already exists for chromosome {chrom}. Skipping.\")\n",
        "        return True\n",
        "\n",
        "    # Run LAMP\n",
        "    print(f\"Running LAMP for Chromosome {chrom}...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    lamp_cmd = f\"{lamp_exe} {config_file}\"\n",
        "    result = subprocess.run(lamp_cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "    # Check if the command was successful\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error running LAMP for chromosome {chrom}:\")\n",
        "        print(result.stderr)\n",
        "        return False\n",
        "\n",
        "    # Check if results file was created\n",
        "    if os.path.exists(results_file):\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "        file_size = os.path.getsize(results_file) / 1024  # Size in KB\n",
        "        print(f\"LAMP completed for chromosome {chrom} in {duration:.1f} seconds.\")\n",
        "        print(f\"Results file size: {file_size:.1f} KB\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"LAMP ran but results file not found for chromosome {chrom}!\")\n",
        "        return False\n",
        "\n",
        "# Print header\n",
        "print(\"=\" * 60)\n",
        "print(\"Step 7: Running LAMP Analysis for Each Chromosome\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check required files before starting\n",
        "print(\"\\nChecking required files before starting LAMP analysis:\")\n",
        "files_ready = True\n",
        "\n",
        "for chrom in range(1, max_chr + 1):\n",
        "    config_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{chrom}_configfile.txt\")\n",
        "    lampgeno_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{chrom}_LAMPGENO.txt\")\n",
        "    lampmap_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{chrom}_LAMPMAP.txt\")\n",
        "    prob_file1 = os.path.join(PROJECT_FOLDER, f\"purebred_{anc_pop1_label}_CHR{chrom}.prob\")\n",
        "    prob_file2 = os.path.join(PROJECT_FOLDER, f\"purebred_{anc_pop2_label}_CHR{chrom}.prob\")\n",
        "\n",
        "    missing_files = []\n",
        "    for file_path, file_type in [\n",
        "        (config_file, \"config\"),\n",
        "        (lampgeno_file, \"LAMPGENO\"),\n",
        "        (lampmap_file, \"LAMPMAP\"),\n",
        "        (prob_file1, f\"{anc_pop1_label} prob\"),\n",
        "        (prob_file2, f\"{anc_pop2_label} prob\")\n",
        "    ]:\n",
        "        if not os.path.exists(file_path):\n",
        "            missing_files.append(file_type)\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"  Chromosome {chrom}: Missing {', '.join(missing_files)} files\")\n",
        "        files_ready = False\n",
        "    else:\n",
        "        print(f\"  Chromosome {chrom}: All required files present ✓\")\n",
        "\n",
        "if not files_ready:\n",
        "    print(\"\\nWarning: Some required files are missing. Make sure to complete Step 6 (Prepare LAMP Input Files) first.\")\n",
        "    print(\"Proceeding anyway, but some chromosomes might be skipped.\")\n",
        "\n",
        "# Run LAMP for each chromosome\n",
        "print(\"\\nRunning LAMP analysis for each chromosome:\")\n",
        "successful_chrs = []\n",
        "failed_chrs = []\n",
        "\n",
        "for chrom in range(1, max_chr + 1):\n",
        "    print(f\"\\nProcessing chromosome {chrom}...\")\n",
        "    success = run_lamp_for_chromosome(chrom)\n",
        "    if success:\n",
        "        successful_chrs.append(chrom)\n",
        "    else:\n",
        "        failed_chrs.append(chrom)\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LAMP Analysis Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total chromosomes processed: {max_chr}\")\n",
        "print(f\"Successfully completed: {len(successful_chrs)} chromosomes\")\n",
        "if successful_chrs:\n",
        "    print(f\"  Chromosomes: {', '.join(map(str, successful_chrs))}\")\n",
        "print(f\"Failed: {len(failed_chrs)} chromosomes\")\n",
        "if failed_chrs:\n",
        "    print(f\"  Chromosomes: {', '.join(map(str, failed_chrs))}\")\n",
        "\n",
        "print(\"\\nNext steps:\")\n",
        "if successful_chrs:\n",
        "    print(\"- Proceed to Step 8 (Calculate Delta) to process the LAMP results\")\n",
        "    print(\"- The delta values will be used to identify regions with ancestry enrichment\")\n",
        "else:\n",
        "    print(\"- Revisit the previous steps to ensure all input files are correctly prepared\")\n",
        "    print(\"- Check that the LAMP executable is working properly\")\n",
        "\n",
        "# Create a file with the list of completed chromosomes for the next step\n",
        "completed_file = os.path.join(PROJECT_FOLDER, \"completed_chromosomes.txt\")\n",
        "with open(completed_file, 'w') as f:\n",
        "    f.write('\\n'.join(map(str, successful_chrs)))\n",
        "print(f\"\\nSaved list of successfully completed chromosomes to {os.path.basename(completed_file)}\")"
      ],
      "metadata": {
        "id": "x-YXaJsnHY_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Calculate Delta from LAMP Results (Fixed)\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Use configuration from global_config\n",
        "try:\n",
        "    # Retrieve configuration from global_config\n",
        "    input_prefix    = global_config[\"file_prefix\"]\n",
        "    filtered_prefix = input_prefix + \"_filtered\"\n",
        "    target_label    = global_config[\"target\"]\n",
        "    anc_pop1_label  = global_config[\"anc1\"]\n",
        "    anc_pop2_label  = global_config[\"anc2\"]\n",
        "    max_chr         = global_config[\"chr_set\"]\n",
        "except NameError:\n",
        "    raise ValueError(\"global_config not found. Please run the interactive parameter cell (Step 2) first.\")\n",
        "\n",
        "# Print header\n",
        "print(\"=\" * 60)\n",
        "print(\"Step 8: Calculate Delta from LAMP Results (Fixed)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Determine which chromosomes have completed LAMP analysis\n",
        "completed_chrs = []\n",
        "for i in range(1, max_chr + 1):\n",
        "    results_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{i}_results.txt\")\n",
        "    if os.path.exists(results_file):\n",
        "        completed_chrs.append(i)\n",
        "\n",
        "if not completed_chrs:\n",
        "    raise FileNotFoundError(\"No LAMP results files found. Please run Step 7 (LAMP Analysis) first.\")\n",
        "\n",
        "print(f\"Found LAMP results for {len(completed_chrs)} chromosomes: {', '.join(map(str, completed_chrs))}\")\n",
        "\n",
        "# Step 1: Determine the number of target individuals\n",
        "fam_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}.fam\")\n",
        "fam_cols = [\"pop\", \"iid\", \"father\", \"mother\", \"sex\", \"phenotype\"]\n",
        "if not os.path.exists(fam_file):\n",
        "    raise FileNotFoundError(f\"FAM file not found: {fam_file}\")\n",
        "\n",
        "fam_df = pd.read_csv(fam_file, sep=r\"\\s+\", header=None, names=fam_cols)\n",
        "target_df = fam_df[fam_df[\"pop\"] == target_label]\n",
        "n_target = target_df.shape[0]\n",
        "print(f\"Found {n_target} target individuals in {os.path.basename(fam_file)} (population: {target_label}).\")\n",
        "\n",
        "# Looking at the LAMP results file format:\n",
        "# - Each individual has 2 rows (one for each haplotype)\n",
        "# - The first column has \"ID:\" followed by tab-separated values\n",
        "# - We need to parse this manually\n",
        "\n",
        "# Function to parse LAMP results file\n",
        "def parse_lamp_results(results_file, odd_indices_only=True):\n",
        "    \"\"\"\n",
        "    Parse LAMP results file, which has the format:\n",
        "    ID: val1 val2 val3 ...\n",
        "\n",
        "    Parameters:\n",
        "    - results_file: Path to the LAMP results file\n",
        "    - odd_indices_only: If True, only return odd-indexed rows (first haplotype of each individual)\n",
        "\n",
        "    Returns:\n",
        "    DataFrame with ancestry values\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(results_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Extract data rows\n",
        "        data_rows = []\n",
        "        for line in lines:\n",
        "            if line.strip():  # Skip empty lines\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) > 1:\n",
        "                    # Extract ID and values\n",
        "                    id_part = parts[0].strip()\n",
        "                    id_num = int(id_part.split(':')[0]) if ':' in id_part else -1\n",
        "                    values = [float(v) for v in parts[1:]]\n",
        "                    data_rows.append((id_num, values))\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        if not data_rows:\n",
        "            return None\n",
        "\n",
        "        # If we only want odd-indexed rows (for the first haplotype of each individual)\n",
        "        if odd_indices_only:\n",
        "            # Find the pattern of indices\n",
        "            unique_ids = sorted(set(row[0] for row in data_rows))\n",
        "            if len(unique_ids) > 1:\n",
        "                # Determine if we have a pattern of 2 rows per individual\n",
        "                # If so, take only the first row of each pair\n",
        "                rows_per_individual = 2\n",
        "                selected_rows = data_rows[::rows_per_individual]\n",
        "            else:\n",
        "                # If we can't detect a pattern, just return all rows\n",
        "                selected_rows = data_rows\n",
        "        else:\n",
        "            selected_rows = data_rows\n",
        "\n",
        "        # Extract just the values\n",
        "        values_only = [row[1] for row in selected_rows]\n",
        "        return pd.DataFrame(values_only)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing LAMP results file: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Step 2: Calculate global mean ancestry across all chromosomes\n",
        "print(\"\\nCalculating global mean ancestry across all chromosomes...\")\n",
        "all_snp_means = []\n",
        "\n",
        "# First pass: Collect all SNP means from all chromosomes\n",
        "for chr_num in completed_chrs:\n",
        "    print(f\"  Processing chromosome {chr_num}... \", end=\"\", flush=True)\n",
        "\n",
        "    # Build file names using filtered_prefix\n",
        "    results_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{chr_num}_results.txt\")\n",
        "\n",
        "    # Parse the LAMP results file\n",
        "    res_df = parse_lamp_results(results_file)\n",
        "    if res_df is None or res_df.empty:\n",
        "        print(\"Error: Could not parse results file or file is empty\")\n",
        "        continue\n",
        "\n",
        "    # Calculate mean ancestry across individuals\n",
        "    snp_means = res_df.mean(axis=0)\n",
        "    all_snp_means.extend(snp_means.tolist())\n",
        "    print(f\"added {len(snp_means)} SNP means\")\n",
        "\n",
        "# Calculate the global mean from all SNP means across all chromosomes\n",
        "if all_snp_means:\n",
        "    global_mean = np.mean(all_snp_means)\n",
        "    global_sd = np.std(all_snp_means)\n",
        "    print(f\"\\nGlobal mean ancestry across all chromosomes: {global_mean:.4f}\")\n",
        "    print(f\"Global standard deviation: {global_sd:.4f}\")\n",
        "else:\n",
        "    print(\"No SNP means were collected. Cannot calculate global mean.\")\n",
        "    global_mean = 0\n",
        "    global_sd = 0\n",
        "\n",
        "# Step 3: Process Each Chromosome to Compute Delta using the global mean\n",
        "merged_list = []\n",
        "\n",
        "for chr_num in completed_chrs:\n",
        "    print(f\"\\n=== Processing Chromosome {chr_num} ===\")\n",
        "\n",
        "    # Build file names using filtered_prefix\n",
        "    results_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{chr_num}_results.txt\")\n",
        "    lampmap_file = os.path.join(PROJECT_FOLDER, f\"{filtered_prefix}_CHR{chr_num}_LAMPMAP.txt\")\n",
        "\n",
        "    if not os.path.exists(results_file) or not os.path.exists(lampmap_file):\n",
        "        print(f\"Required files not found for chromosome {chr_num}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Parse the LAMP results file\n",
        "    res_df = parse_lamp_results(results_file)\n",
        "    if res_df is None or res_df.empty:\n",
        "        print(f\"Error: Could not parse results file for chromosome {chr_num} or file is empty\")\n",
        "        continue\n",
        "\n",
        "    # Calculate mean ancestry across individuals\n",
        "    snp_means = res_df.mean(axis=0)\n",
        "\n",
        "    # Calculate delta using the global mean across all chromosomes\n",
        "    delta = snp_means - global_mean\n",
        "\n",
        "    # Read positions from LAMPMAP file\n",
        "    try:\n",
        "        lampmap = pd.read_csv(lampmap_file, sep=\"\\t\", header=None)\n",
        "        if lampmap.shape[1] != 1:\n",
        "            print(f\"Warning: Unexpected format in LAMPMAP file for chromosome {chr_num}\")\n",
        "\n",
        "        # Convert positions from basepairs to megabases (Mb)\n",
        "        lampmap[0] = lampmap[0] / 1e6\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading LAMPMAP file for chromosome {chr_num}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "    # Check if the number of SNPs matches; if not, truncate to the minimum length\n",
        "    len_delta = len(delta)\n",
        "    len_map = len(lampmap)\n",
        "    if len_delta != len_map:\n",
        "        print(f\"Warning: Mismatch in SNP counts for chromosome {chr_num}: {len_delta} vs. {len_map}. Truncating to minimum length.\")\n",
        "        min_length = min(len_delta, len_map)\n",
        "        delta = delta.iloc[:min_length]\n",
        "        lampmap = lampmap.iloc[:min_length]\n",
        "\n",
        "    # Create a DataFrame with delta values and SNP positions\n",
        "    chr_delta_df = pd.DataFrame({\n",
        "        \"admix_rate\": delta.values,\n",
        "        \"position\": lampmap[0].values,\n",
        "        \"chromosome\": chr_num\n",
        "    })\n",
        "\n",
        "    # Save per-chromosome delta file\n",
        "    out_filename = os.path.join(PROJECT_FOLDER, f\"{target_label}_{anc_pop1_label}_{anc_pop2_label}_CHR{chr_num}.txt\")\n",
        "    chr_delta_df.to_csv(out_filename, sep=\"\\t\", index=False)\n",
        "    print(f\"Saved delta file for chromosome {chr_num} as {os.path.basename(out_filename)}\")\n",
        "\n",
        "    merged_list.append(chr_delta_df)\n",
        "\n",
        "# Step 4: Merge All Chromosome Delta Files into One\n",
        "if merged_list:\n",
        "    merged_df = pd.concat(merged_list, ignore_index=True)\n",
        "    merged_output = os.path.join(PROJECT_FOLDER, f\"{target_label}_{anc_pop1_label}_{anc_pop2_label}_merged.txt\")\n",
        "    merged_df.to_csv(merged_output, sep=\"\\t\", index=False)\n",
        "    print(f\"\\nMerged delta file created: {os.path.basename(merged_output)}\")\n",
        "    print(f\"Total SNPs in merged file: {merged_df.shape[0]}\")\n",
        "\n",
        "    # Display the first few rows of the merged file to confirm format\n",
        "    print(\"\\nFirst few rows of the merged file:\")\n",
        "    print(merged_df.head().to_string())\n",
        "\n",
        "    # Calculate summary statistics for the deltas\n",
        "    mean_delta = merged_df['admix_rate'].mean()\n",
        "    sd_delta = merged_df['admix_rate'].std()\n",
        "    min_delta = merged_df['admix_rate'].min()\n",
        "    max_delta = merged_df['admix_rate'].max()\n",
        "\n",
        "    print(\"\\nDelta Statistics:\")\n",
        "    print(f\"  Mean: {mean_delta:.4f}\")\n",
        "    print(f\"  Standard Deviation: {sd_delta:.4f}\")\n",
        "    print(f\"  Min: {min_delta:.4f}\")\n",
        "    print(f\"  Max: {max_delta:.4f}\")\n",
        "\n",
        "    # Calculate enrichment thresholds\n",
        "    threshold_2sd = 2 * sd_delta\n",
        "    threshold_3sd = 3 * sd_delta\n",
        "\n",
        "    print(f\"\\nEnrichment Thresholds:\")\n",
        "    print(f\"  2 SD (+/-): {threshold_2sd:.4f}\")\n",
        "    print(f\"  3 SD (+/-): {threshold_3sd:.4f}\")\n",
        "\n",
        "    # Count regions exceeding thresholds\n",
        "    exceed_2sd = merged_df[abs(merged_df['admix_rate']) > threshold_2sd]\n",
        "    exceed_3sd = merged_df[abs(merged_df['admix_rate']) > threshold_3sd]\n",
        "\n",
        "    print(f\"\\nRegions exceeding thresholds:\")\n",
        "    print(f\"  > 2 SD: {exceed_2sd.shape[0]} SNPs ({exceed_2sd.shape[0]/merged_df.shape[0]*100:.2f}%)\")\n",
        "    print(f\"  > 3 SD: {exceed_3sd.shape[0]} SNPs ({exceed_3sd.shape[0]/merged_df.shape[0]*100:.2f}%)\")\n",
        "\n",
        "    # Save threshold values for the next step\n",
        "    thresholds_file = os.path.join(PROJECT_FOLDER, \"delta_thresholds.txt\")\n",
        "    with open(thresholds_file, 'w') as f:\n",
        "        f.write(f\"mean\\t{mean_delta}\\n\")\n",
        "        f.write(f\"sd\\t{sd_delta}\\n\")\n",
        "        f.write(f\"threshold_2sd\\t{threshold_2sd}\\n\")\n",
        "        f.write(f\"threshold_3sd\\t{threshold_3sd}\\n\")\n",
        "\n",
        "    print(f\"\\nThreshold values saved to {os.path.basename(thresholds_file)}\")\n",
        "else:\n",
        "    print(\"No delta data available to merge.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sJpVaFAXIl6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Visualization and Identification of Significant Regions\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Use configuration from global_config\n",
        "try:\n",
        "    # Retrieve configuration from global_config\n",
        "    input_prefix    = global_config[\"file_prefix\"]\n",
        "    filtered_prefix = input_prefix + \"_filtered\"\n",
        "    target_label    = global_config[\"target\"]\n",
        "    anc_pop1_label  = global_config[\"anc1\"]\n",
        "    anc_pop2_label  = global_config[\"anc2\"]\n",
        "    max_chr         = global_config[\"chr_set\"]\n",
        "except NameError:\n",
        "    raise ValueError(\"global_config not found. Please run the interactive parameter cell (Step 2) first.\")\n",
        "\n",
        "# Print header\n",
        "print(\"=\" * 60)\n",
        "print(\"Step 9: Visualization and Identification of Significant Regions\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Build the merged file name according to previous step output\n",
        "merged_filename = os.path.join(PROJECT_FOLDER, f\"{target_label}_{anc_pop1_label}_{anc_pop2_label}_merged.txt\")\n",
        "if not os.path.exists(merged_filename):\n",
        "    raise FileNotFoundError(f\"Merged delta file not found: {merged_filename}\")\n",
        "\n",
        "# Load the merged delta data\n",
        "print(f\"Loading merged delta data from {os.path.basename(merged_filename)}\")\n",
        "data = pd.read_csv(merged_filename, sep=\"\\t\")\n",
        "print(f\"Loaded {data.shape[0]} SNPs across {data['chromosome'].nunique()} chromosomes\")\n",
        "\n",
        "# Define a function to process and save data for significant regions\n",
        "def identify_significant_regions(chromosome_data):\n",
        "    # Calculate the mean and standard deviation of admix_rate\n",
        "    mean_admix = chromosome_data['admix_rate'].mean()\n",
        "    sd_admix = chromosome_data['admix_rate'].std()\n",
        "\n",
        "    # Set the threshold to 3 standard deviations\n",
        "    threshold = 3 * sd_admix\n",
        "\n",
        "    # Identify rows exceeding the threshold\n",
        "    exceed_threshold = chromosome_data[abs(chromosome_data['admix_rate']) > threshold]\n",
        "\n",
        "    return exceed_threshold, threshold\n",
        "\n",
        "# Create directory for output files\n",
        "output_dir = os.path.join(PROJECT_FOLDER, \"significant_regions\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Saving significant region results to {output_dir}\")\n",
        "\n",
        "# Process each chromosome and identify significant regions\n",
        "print(\"\\nIdentifying significant regions by chromosome:\")\n",
        "all_significant_regions = []\n",
        "\n",
        "for chromosome in sorted(data['chromosome'].unique()):\n",
        "    chromosome_data = data[data['chromosome'] == chromosome]\n",
        "    exceed_threshold, threshold = identify_significant_regions(chromosome_data)\n",
        "\n",
        "    if not exceed_threshold.empty:\n",
        "        # Save to file\n",
        "        file_name = os.path.join(output_dir, f\"chromosome_{chromosome}_exceed_threshold.txt\")\n",
        "        exceed_threshold.to_csv(file_name, sep=\"\\t\", index=False, quoting=3)\n",
        "\n",
        "        # Add to combined results\n",
        "        all_significant_regions.append(exceed_threshold)\n",
        "\n",
        "        print(f\"  Chromosome {chromosome}: {len(exceed_threshold)} significant SNPs (threshold = {threshold:.4f})\")\n",
        "    else:\n",
        "        print(f\"  Chromosome {chromosome}: No significant SNPs found\")\n",
        "\n",
        "# Combine all significant regions into one file\n",
        "if all_significant_regions:\n",
        "    combined = pd.concat(all_significant_regions, ignore_index=True)\n",
        "    combined_file = os.path.join(output_dir, \"all_significant_regions.txt\")\n",
        "    combined.to_csv(combined_file, sep=\"\\t\", index=False, quoting=3)\n",
        "    print(f\"\\nCombined all significant regions into {os.path.basename(combined_file)}\")\n",
        "    print(f\"Total significant SNPs across all chromosomes: {len(combined)}\")\n",
        "else:\n",
        "    print(\"\\nNo significant regions found in any chromosome\")\n",
        "\n",
        "# Create directory for plots\n",
        "plots_dir = os.path.join(PROJECT_FOLDER, \"chromosome_plots\")\n",
        "os.makedirs(plots_dir, exist_ok=True)\n",
        "print(f\"\\nPreparing multi-chromosome grid plots...\")\n",
        "\n",
        "# Define a function to plot chromosome in grid layout\n",
        "def plot_chromosome_in_grid(ax, chromosome_data, chromosome):\n",
        "    # Calculate the mean and standard deviation of admix_rate\n",
        "    mean_admix = chromosome_data['admix_rate'].mean()\n",
        "    sd_admix = chromosome_data['admix_rate'].std()\n",
        "    threshold = 3 * sd_admix\n",
        "\n",
        "    # Create the line plot\n",
        "    ax.plot(chromosome_data['position'], chromosome_data['admix_rate'], color='blue')\n",
        "\n",
        "    # Highlight regions exceeding the threshold\n",
        "    exceed_threshold = chromosome_data[abs(chromosome_data['admix_rate']) > threshold]\n",
        "    if not exceed_threshold.empty:\n",
        "        ax.scatter(exceed_threshold['position'], exceed_threshold['admix_rate'], color='red')\n",
        "\n",
        "    # Add zero line (dotted green)\n",
        "    ax.axhline(y=0, color='green', linestyle=':', alpha=0.8)\n",
        "\n",
        "    # Add threshold lines (dashed orange) without values\n",
        "    ax.axhline(y=threshold, color='orange', linestyle='--')\n",
        "    ax.axhline(y=-threshold, color='orange', linestyle='--')\n",
        "\n",
        "    # Set x-axis labels and avoid overlapping\n",
        "    min_pos = chromosome_data['position'].min()\n",
        "    max_pos = chromosome_data['position'].max()\n",
        "    # Use first, last and either middle point or space them evenly\n",
        "    if max_pos - min_pos > 50:\n",
        "        positions = [min_pos, min_pos + (max_pos - min_pos)/2, max_pos]\n",
        "    else:\n",
        "        positions = [min_pos, max_pos]\n",
        "\n",
        "    positions = [round(pos, 2) for pos in positions]\n",
        "\n",
        "    # Format labels to prevent overlapping\n",
        "    if max(positions) > 100:\n",
        "        x_labels = [f\"{p:.0f}\" for p in positions]  # No decimal for large values\n",
        "    else:\n",
        "        x_labels = [f\"{p:.2f}\" for p in positions]\n",
        "\n",
        "    ax.set_xticks(positions)\n",
        "    ax.set_xticklabels(x_labels, fontsize=8)\n",
        "\n",
        "    # Set title\n",
        "    ax.set_title(f\"Chromosome {chromosome}\")\n",
        "\n",
        "    # Set axis labels\n",
        "    ax.set_xlabel(\"Position (cM)\", fontsize=8)\n",
        "    if ax.get_subplotspec().is_first_col():  # Only for leftmost column\n",
        "        ax.set_ylabel(\"Delta Ancestry\", fontsize=8)\n",
        "\n",
        "    # Set y-axis limits to accommodate thresholds plus a little padding\n",
        "    max_y = max(threshold * 1.2, chromosome_data['admix_rate'].max() * 1.2)\n",
        "    min_y = min(-threshold * 1.2, chromosome_data['admix_rate'].min() * 1.2)\n",
        "    ax.set_ylim(min_y, max_y)\n",
        "\n",
        "    # Add a faint border\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_linewidth(0.5)\n",
        "        spine.set_color('#888888')\n",
        "\n",
        "# Create multi-chromosome grid plots\n",
        "print(\"\\nCreating multi-chromosome grid plots...\")\n",
        "chromosomes = sorted(data['chromosome'].unique())\n",
        "num_chromosomes = len(chromosomes)\n",
        "chromosomes_per_page = 16  # 16 per page (4x4 grid)\n",
        "\n",
        "# Calculate total number of pages needed\n",
        "total_pages = (num_chromosomes + chromosomes_per_page - 1) // chromosomes_per_page\n",
        "\n",
        "# Define consistent figure size and layout for all pages\n",
        "figsize = (15, 12)\n",
        "\n",
        "# Process all pages with consistent formatting\n",
        "for page in range(total_pages):\n",
        "    page_num = page + 1\n",
        "    start_idx = page * chromosomes_per_page\n",
        "    end_idx = min(start_idx + chromosomes_per_page, num_chromosomes)\n",
        "\n",
        "    # Create a figure with consistent dimensions for all pages\n",
        "    fig, axes = plt.subplots(4, 4, figsize=figsize)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Process chromosomes for this page\n",
        "    for i, idx in enumerate(range(start_idx, end_idx)):\n",
        "        chromosome = chromosomes[idx]\n",
        "        chromosome_data = data[data['chromosome'] == chromosome]\n",
        "        plot_chromosome_in_grid(axes[i], chromosome_data, chromosome)\n",
        "\n",
        "    # Set empty plots for any remaining slots\n",
        "    for j in range(end_idx - start_idx, 16):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    # Add title with consistent formatting\n",
        "    plt.suptitle(f\"Ancestry Deviation by Chromosome - Page {page_num}\", fontsize=14, y=0.98)\n",
        "\n",
        "    # Add legend only to the first plot of each page\n",
        "    if page_num == 1:  # For first page only\n",
        "        # Add a legend to the top right of the figure\n",
        "        legend_elements = [\n",
        "            plt.Line2D([0], [0], color='orange', linestyle='--', label='3SD Threshold'),\n",
        "            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=8, label='Exceed 3SD Threshold')\n",
        "        ]\n",
        "        fig.legend(handles=legend_elements, loc='upper right', ncol=2, frameon=False,\n",
        "                  bbox_to_anchor=(0.98, 0.98), fontsize=10)\n",
        "\n",
        "    # Ensure consistent spacing and layout\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    # Save the grid plot with consistent naming\n",
        "    output_file = os.path.join(plots_dir, f\"chromosomes_grid_page_{page_num}.png\")\n",
        "    plt.savefig(output_file, dpi=300)\n",
        "    print(f\"  Created grid plot page {page_num}: {os.path.basename(output_file)}\")\n",
        "    plt.close()\n",
        "\n",
        "print(\"\\nVisualization complete. All plots saved successfully.\")\n",
        "\n",
        "# Create a summary file with counts of significant regions by chromosome\n",
        "summary_data = []\n",
        "for chromosome in sorted(data['chromosome'].unique()):\n",
        "    chromosome_data = data[data['chromosome'] == chromosome]\n",
        "    exceed_threshold, threshold = identify_significant_regions(chromosome_data)\n",
        "\n",
        "    summary_data.append({\n",
        "        'Chromosome': chromosome,\n",
        "        'Total_SNPs': len(chromosome_data),\n",
        "        'Significant_SNPs': len(exceed_threshold),\n",
        "        'Threshold': threshold,\n",
        "        'Percent_Significant': (len(exceed_threshold) / len(chromosome_data) * 100) if len(chromosome_data) > 0 else 0\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_file = os.path.join(PROJECT_FOLDER, \"significance_summary.txt\")\n",
        "summary_df.to_csv(summary_file, sep=\"\\t\", index=False)\n",
        "print(f\"\\nSaved significance summary to {os.path.basename(summary_file)}\")"
      ],
      "metadata": {
        "id": "Yl-cPzpAQcSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Final Analysis and Reporting\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "# Use configuration from global_config\n",
        "try:\n",
        "    # Retrieve configuration from global_config\n",
        "    input_prefix    = global_config[\"file_prefix\"]\n",
        "    filtered_prefix = input_prefix + \"_filtered\"\n",
        "    target_label    = global_config[\"target\"]\n",
        "    anc_pop1_label  = global_config[\"anc1\"]\n",
        "    anc_pop2_label  = global_config[\"anc2\"]\n",
        "    max_chr         = global_config[\"chr_set\"]\n",
        "except NameError:\n",
        "    raise ValueError(\"global_config not found. Please run the interactive parameter cell (Step 2) first.\")\n",
        "\n",
        "# Print header\n",
        "print(\"=\" * 60)\n",
        "print(\"Step 10: Final Analysis and Reporting\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define important file paths\n",
        "merged_file = os.path.join(PROJECT_FOLDER, f\"{target_label}_{anc_pop1_label}_{anc_pop2_label}_merged.txt\")\n",
        "significant_file = os.path.join(PROJECT_FOLDER, \"significant_regions\", \"all_significant_regions.txt\")\n",
        "summary_file = os.path.join(PROJECT_FOLDER, \"significance_summary.txt\")\n",
        "\n",
        "# Check if required files exist\n",
        "required_files = [merged_file, significant_file, summary_file]\n",
        "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"Warning: The following required files are missing: {', '.join(missing_files)}\")\n",
        "    print(\"Some analyses may be incomplete. Please run previous steps first.\")\n",
        "else:\n",
        "    print(\"All required input files found.\")\n",
        "\n",
        "# Create directory for final outputs\n",
        "final_dir = os.path.join(PROJECT_FOLDER, \"final_results\")\n",
        "os.makedirs(final_dir, exist_ok=True)\n",
        "print(f\"Final results will be saved to: {final_dir}\")\n",
        "\n",
        "# Load the merged data file\n",
        "try:\n",
        "    print(f\"\\nLoading merged data from {os.path.basename(merged_file)}...\")\n",
        "    merged_data = pd.read_csv(merged_file, sep=\"\\t\")\n",
        "    print(f\"Loaded {merged_data.shape[0]} SNPs across {merged_data['chromosome'].nunique()} chromosomes\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading merged data: {str(e)}\")\n",
        "    merged_data = None\n",
        "\n",
        "# Load the significant regions file if it exists\n",
        "try:\n",
        "    print(f\"\\nLoading significant regions from {os.path.basename(significant_file)}...\")\n",
        "    sig_regions = pd.read_csv(significant_file, sep=\"\\t\")\n",
        "    print(f\"Loaded {sig_regions.shape[0]} significant SNPs\")\n",
        "except Exception as e:\n",
        "    print(f\"Error or no significant regions file: {str(e)}\")\n",
        "    sig_regions = None\n",
        "\n",
        "# Load the summary file\n",
        "try:\n",
        "    print(f\"\\nLoading significance summary from {os.path.basename(summary_file)}...\")\n",
        "    summary = pd.read_csv(summary_file, sep=\"\\t\")\n",
        "    print(f\"Loaded summary statistics for {summary.shape[0]} chromosomes\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading summary file: {str(e)}\")\n",
        "    summary = None\n",
        "\n",
        "# PART 1: Create chromosome-wide heatmap visualization\n",
        "if merged_data is not None:\n",
        "    print(\"\\n1. Creating chromosome-wide heatmap visualization...\")\n",
        "\n",
        "    # Prepare data for heatmap\n",
        "    chromosomes = sorted(merged_data['chromosome'].unique())\n",
        "\n",
        "    # Create a figure with a heatmap-style representation\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # For each chromosome, calculate the percentage of significant regions in bins\n",
        "    bin_size = 1  # in Mb (adjust as needed)\n",
        "    max_chr_length = 0\n",
        "    all_binned_data = []\n",
        "\n",
        "    for chromosome in chromosomes:\n",
        "        chr_data = merged_data[merged_data['chromosome'] == chromosome]\n",
        "        if chr_data.empty:\n",
        "            continue\n",
        "\n",
        "        # Find chromosome length\n",
        "        chr_length = int(np.ceil(chr_data['position'].max()))\n",
        "        max_chr_length = max(max_chr_length, chr_length)\n",
        "\n",
        "        # Create bins\n",
        "        bins = np.arange(0, chr_length + bin_size, bin_size)\n",
        "        binned_data = np.zeros(len(bins) - 1)\n",
        "\n",
        "        # Calculate mean delta for each bin\n",
        "        for i in range(len(bins) - 1):\n",
        "            bin_start, bin_end = bins[i], bins[i+1]\n",
        "            bin_snps = chr_data[(chr_data['position'] >= bin_start) & (chr_data['position'] < bin_end)]\n",
        "\n",
        "            if not bin_snps.empty:\n",
        "                binned_data[i] = bin_snps['admix_rate'].mean()\n",
        "\n",
        "        # Pad with zeros to make all chromosomes the same length\n",
        "        all_binned_data.append(np.pad(binned_data, (0, int(np.ceil(max_chr_length/bin_size)) - len(binned_data)), 'constant'))\n",
        "\n",
        "    # Create a heatmap\n",
        "    heatmap_data = np.array(all_binned_data)\n",
        "\n",
        "    # Create a custom colormap with white at zero\n",
        "    max_abs_val = np.max(np.abs(heatmap_data))\n",
        "    colors = [(0, 0, 1), (1, 1, 1), (1, 0, 0)]  # Blue -> White -> Red\n",
        "    cmap = LinearSegmentedColormap.from_list(\"BlueWhiteRed\", colors, N=256)\n",
        "\n",
        "    # Plot heatmap\n",
        "    plt.imshow(heatmap_data, aspect='auto', cmap=cmap, vmin=-max_abs_val, vmax=max_abs_val)\n",
        "    plt.colorbar(label=f\"Mean Delta Ancestry ({anc_pop1_label} vs {anc_pop2_label})\")\n",
        "\n",
        "    # Set y-axis ticks to chromosome numbers\n",
        "    plt.yticks(range(len(chromosomes)), [f\"Chr {c}\" for c in chromosomes])\n",
        "\n",
        "    # Set x-axis to position in Mb\n",
        "    tick_positions = np.arange(0, int(np.ceil(max_chr_length/bin_size)), 10)  # Every 10 Mb\n",
        "    plt.xticks(tick_positions, [f\"{p*bin_size}\" for p in tick_positions])\n",
        "    plt.xlabel(\"Position (Mb)\")\n",
        "\n",
        "    plt.title(f\"Genome-wide Ancestry Deviations in {target_label}\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure\n",
        "    heatmap_file = os.path.join(final_dir, \"genome_wide_heatmap.png\")\n",
        "    plt.savefig(heatmap_file, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"Saved genome-wide heatmap to {os.path.basename(heatmap_file)}\")\n",
        "\n",
        "# PART 2: Analyze significant regions and create summary statistics\n",
        "if sig_regions is not None:\n",
        "    print(\"\\n2. Analyzing significant regions...\")\n",
        "\n",
        "    # Count significant regions per chromosome and direction of deviation\n",
        "    chromosome_stats = []\n",
        "\n",
        "    for chromosome in sorted(sig_regions['chromosome'].unique()):\n",
        "        chr_sig = sig_regions[sig_regions['chromosome'] == chromosome]\n",
        "\n",
        "        # Count positive and negative deviations\n",
        "        positive_count = sum(chr_sig['admix_rate'] > 0)\n",
        "        negative_count = sum(chr_sig['admix_rate'] < 0)\n",
        "\n",
        "        # Calculate the percentage of positive vs negative\n",
        "        total = positive_count + negative_count\n",
        "        pos_percent = (positive_count / total * 100) if total > 0 else 0\n",
        "        neg_percent = (negative_count / total * 100) if total > 0 else 0\n",
        "\n",
        "        # Get statistics on the magnitude of deviations\n",
        "        mean_positive = chr_sig[chr_sig['admix_rate'] > 0]['admix_rate'].mean() if positive_count > 0 else 0\n",
        "        mean_negative = chr_sig[chr_sig['admix_rate'] < 0]['admix_rate'].mean() if negative_count > 0 else 0\n",
        "        max_positive = chr_sig['admix_rate'].max() if positive_count > 0 else 0\n",
        "        min_negative = chr_sig['admix_rate'].min() if negative_count > 0 else 0\n",
        "\n",
        "        chromosome_stats.append({\n",
        "            'Chromosome': chromosome,\n",
        "            f'{anc_pop1_label}_enriched': positive_count,\n",
        "            f'{anc_pop2_label}_enriched': negative_count,\n",
        "            'Total_significant': total,\n",
        "            f'{anc_pop1_label}_percent': pos_percent,\n",
        "            f'{anc_pop2_label}_percent': neg_percent,\n",
        "            f'Mean_{anc_pop1_label}_enrichment': mean_positive,\n",
        "            f'Mean_{anc_pop2_label}_enrichment': mean_negative,\n",
        "            f'Max_{anc_pop1_label}_enrichment': max_positive,\n",
        "            f'Min_{anc_pop2_label}_enrichment': min_negative\n",
        "        })\n",
        "\n",
        "    # Create a DataFrame and save\n",
        "    chr_stats_df = pd.DataFrame(chromosome_stats)\n",
        "    chr_stats_file = os.path.join(final_dir, \"chromosome_statistics.txt\")\n",
        "    chr_stats_df.to_csv(chr_stats_file, sep=\"\\t\", index=False)\n",
        "    print(f\"Saved chromosome statistics to {os.path.basename(chr_stats_file)}\")\n",
        "\n",
        "    # Identify clusters of significant regions (potential selective sweeps)\n",
        "    print(\"\\n3. Identifying clusters of significant regions...\")\n",
        "\n",
        "    # Group adjacent significant SNPs into clusters\n",
        "    clusters = []\n",
        "    current_cluster = []\n",
        "\n",
        "    # Sort significant regions by chromosome and position\n",
        "    sorted_sig = sig_regions.sort_values(['chromosome', 'position'])\n",
        "\n",
        "    # Set distance threshold for considering SNPs part of the same cluster (e.g., 1 Mb)\n",
        "    distance_threshold = 1.0  # in Mb\n",
        "\n",
        "    # Iterate through SNPs\n",
        "    for i, row in sorted_sig.iterrows():\n",
        "        if not current_cluster:\n",
        "            # Start a new cluster with the first SNP\n",
        "            current_cluster = [row]\n",
        "        else:\n",
        "            prev_row = current_cluster[-1]\n",
        "\n",
        "            # Check if this SNP belongs to the current cluster\n",
        "            if (row['chromosome'] == prev_row['chromosome'] and\n",
        "                row['position'] - prev_row['position'] <= distance_threshold):\n",
        "                current_cluster.append(row)\n",
        "            else:\n",
        "                # Save the completed cluster and start a new one\n",
        "                if len(current_cluster) >= 3:  # Only keep clusters with at least 3 SNPs\n",
        "                    clusters.append(pd.DataFrame(current_cluster))\n",
        "                current_cluster = [row]\n",
        "\n",
        "    # Don't forget the last cluster\n",
        "    if current_cluster and len(current_cluster) >= 3:\n",
        "        clusters.append(pd.DataFrame(current_cluster))\n",
        "\n",
        "    # Save information about clusters\n",
        "    if clusters:\n",
        "        print(f\"Found {len(clusters)} clusters of significant regions\")\n",
        "\n",
        "        # Save each cluster to a separate file\n",
        "        os.makedirs(os.path.join(final_dir, \"clusters\"), exist_ok=True)\n",
        "\n",
        "        # Create a summary of clusters\n",
        "        cluster_summary = []\n",
        "\n",
        "        for i, cluster in enumerate(clusters):\n",
        "            # Calculate cluster statistics\n",
        "            start_pos = cluster['position'].min()\n",
        "            end_pos = cluster['position'].max()\n",
        "            length = end_pos - start_pos\n",
        "            chromosome = cluster['chromosome'].iloc[0]\n",
        "            avg_delta = cluster['admix_rate'].mean()\n",
        "            direction = f\"{anc_pop1_label} enriched\" if avg_delta > 0 else f\"{anc_pop2_label} enriched\"\n",
        "\n",
        "            # Save cluster details\n",
        "            cluster_file = os.path.join(final_dir, \"clusters\", f\"cluster_{i+1}_chr{chromosome}.txt\")\n",
        "            cluster.to_csv(cluster_file, sep=\"\\t\", index=False)\n",
        "\n",
        "            cluster_summary.append({\n",
        "                'Cluster_ID': i+1,\n",
        "                'Chromosome': chromosome,\n",
        "                'Start_Position': start_pos,\n",
        "                'End_Position': end_pos,\n",
        "                'Length_Mb': length,\n",
        "                'SNP_Count': len(cluster),\n",
        "                'Mean_Delta': avg_delta,\n",
        "                'Direction': direction\n",
        "            })\n",
        "\n",
        "        # Create a summary DataFrame and save\n",
        "        cluster_summary_df = pd.DataFrame(cluster_summary)\n",
        "        cluster_summary_file = os.path.join(final_dir, \"cluster_summary.txt\")\n",
        "        cluster_summary_df.to_csv(cluster_summary_file, sep=\"\\t\", index=False)\n",
        "        print(f\"Saved cluster summary to {os.path.basename(cluster_summary_file)}\")\n",
        "\n",
        "        # Create a visualization of the top clusters\n",
        "        top_clusters = cluster_summary_df.sort_values('SNP_Count', ascending=False).head(10)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        colors = ['red' if direction.startswith(anc_pop1_label) else 'blue' for direction in top_clusters['Direction']]\n",
        "\n",
        "        plt.barh(range(len(top_clusters)), top_clusters['SNP_Count'], color=colors)\n",
        "\n",
        "        # Add chromosome and position labels\n",
        "        labels = [f\"Chr{row['Chromosome']}: {row['Start_Position']:.1f}-{row['End_Position']:.1f} Mb\"\n",
        "                for _, row in top_clusters.iterrows()]\n",
        "\n",
        "        plt.yticks(range(len(top_clusters)), labels)\n",
        "        plt.xlabel('Number of Significant SNPs in Cluster')\n",
        "        plt.title('Top 10 Clusters of Significant Ancestry Deviation')\n",
        "\n",
        "        legend_elements = [\n",
        "            plt.Rectangle((0,0), 1, 1, color='red', label=f'{anc_pop1_label} enriched'),\n",
        "            plt.Rectangle((0,0), 1, 1, color='blue', label=f'{anc_pop2_label} enriched')\n",
        "        ]\n",
        "        plt.legend(handles=legend_elements)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the figure\n",
        "        top_clusters_file = os.path.join(final_dir, \"top_clusters.png\")\n",
        "        plt.savefig(top_clusters_file, dpi=300)\n",
        "        plt.close()\n",
        "        print(f\"Saved top clusters visualization to {os.path.basename(top_clusters_file)}\")\n",
        "    else:\n",
        "        print(\"No significant clusters found\")\n",
        "\n",
        "# PART 3: Create a final report\n",
        "print(\"\\n4. Generating final report...\")\n",
        "\n",
        "# Create a final report in HTML format\n",
        "report_file = os.path.join(final_dir, \"final_report.html\")\n",
        "\n",
        "with open(report_file, 'w') as f:\n",
        "    f.write(f'''\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Local Ancestry Analysis Report</title>\n",
        "        <style>\n",
        "            body {{ font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }}\n",
        "            h1, h2, h3 {{ color: #2c3e50; }}\n",
        "            .container {{ max-width: 1200px; margin: 0 auto; }}\n",
        "            table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}\n",
        "            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "            th {{ background-color: #f2f2f2; }}\n",
        "            tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
        "            .highlight {{ background-color: #ffffcc; }}\n",
        "            img {{ max-width: 100%; height: auto; margin: 10px 0; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"container\">\n",
        "            <h1>Local Ancestry Analysis Report</h1>\n",
        "            <p><strong>Date:</strong> {pd.Timestamp.now().strftime('%Y-%m-%d')}</p>\n",
        "\n",
        "            <h2>Analysis Parameters</h2>\n",
        "            <ul>\n",
        "                <li><strong>Target Population:</strong> {target_label}</li>\n",
        "                <li><strong>Ancestral Population 1:</strong> {anc_pop1_label}</li>\n",
        "                <li><strong>Ancestral Population 2:</strong> {anc_pop2_label}</li>\n",
        "                <li><strong>Number of Chromosomes:</strong> {max_chr}</li>\n",
        "            </ul>\n",
        "\n",
        "            <h2>Summary Statistics</h2>\n",
        "    ''')\n",
        "\n",
        "    # Include summary statistics if available\n",
        "    if summary is not None:\n",
        "        total_snps = summary['Total_SNPs'].sum()\n",
        "        total_sig = summary['Significant_SNPs'].sum()\n",
        "        pct_sig = (total_sig / total_snps * 100) if total_snps > 0 else 0\n",
        "\n",
        "        f.write(f'''\n",
        "            <p>Total SNPs analyzed: {total_snps:,}</p>\n",
        "            <p>Significant SNPs: {total_sig:,} ({pct_sig:.2f}%)</p>\n",
        "\n",
        "            <h3>Significant SNPs by Chromosome</h3>\n",
        "            <table>\n",
        "                <tr>\n",
        "                    <th>Chromosome</th>\n",
        "                    <th>Total SNPs</th>\n",
        "                    <th>Significant SNPs</th>\n",
        "                    <th>Percent</th>\n",
        "                    <th>Threshold</th>\n",
        "                </tr>\n",
        "        ''')\n",
        "\n",
        "        for _, row in summary.iterrows():\n",
        "            f.write(f'''\n",
        "                <tr>\n",
        "                    <td>{row['Chromosome']}</td>\n",
        "                    <td>{row['Total_SNPs']:,}</td>\n",
        "                    <td>{row['Significant_SNPs']:,}</td>\n",
        "                    <td>{row['Percent_Significant']:.2f}%</td>\n",
        "                    <td>±{row['Threshold']:.4f}</td>\n",
        "                </tr>\n",
        "            ''')\n",
        "\n",
        "        f.write('</table>')\n",
        "\n",
        "    # Include distribution of enrichment if sig_regions is available\n",
        "    if sig_regions is not None:\n",
        "        positive_count = sum(sig_regions['admix_rate'] > 0)\n",
        "        negative_count = sum(sig_regions['admix_rate'] < 0)\n",
        "        total_sig = positive_count + negative_count\n",
        "\n",
        "        f.write(f'''\n",
        "            <h3>Distribution of Ancestry Enrichment</h3>\n",
        "            <p>{anc_pop1_label} enriched regions: {positive_count:,} ({positive_count/total_sig*100:.2f}%)</p>\n",
        "            <p>{anc_pop2_label} enriched regions: {negative_count:,} ({negative_count/total_sig*100:.2f}%)</p>\n",
        "        ''')\n",
        "\n",
        "    # Include images\n",
        "    f.write('''\n",
        "            <h2>Visualizations</h2>\n",
        "\n",
        "            <h3>Genome-wide Ancestry Deviations</h3>\n",
        "            <img src=\"genome_wide_heatmap.png\" alt=\"Genome-wide heatmap\">\n",
        "\n",
        "            <h3>Top Clusters of Significant Deviation</h3>\n",
        "            <img src=\"top_clusters.png\" alt=\"Top clusters\">\n",
        "    ''')\n",
        "\n",
        "    # Add conclusions\n",
        "    f.write('''\n",
        "            <h2>Conclusions</h2>\n",
        "            <p>This analysis has identified regions of the genome where local ancestry shows significant deviation from the genome-wide average. These regions may represent:</p>\n",
        "            <ul>\n",
        "                <li>Areas under selection during the formation of the target population</li>\n",
        "                <li>Regions with adaptive introgression from one ancestral population</li>\n",
        "                <li>Genomic areas with ancestry-related incompatibilities</li>\n",
        "            </ul>\n",
        "            <p>Further investigation of the specific genes in these regions is recommended.</p>\n",
        "        </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    ''')\n",
        "\n",
        "print(f\"Saved final report to {os.path.basename(report_file)}\")\n",
        "print(\"\\nFinal analysis complete. Results are available in the following directory:\")\n",
        "print(f\"{final_dir}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5h_RA-6qLNjS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}